{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font style=\"color:blue\">Convolutional Neural Network using Batch Normalization</font>\n",
    "\n",
    "In this notebook, we will add batch norm layer to the LeNet network and see what effects does it have on the network training and convergence.\n",
    "\n",
    "Instead of using the MNIST dataset, which overfits easily, we will use Fashion MNIST dataset. \n",
    "\n",
    "The figure below shows some samples from the Fashion MNIST dataset.\n",
    "\n",
    "<img src=\"https://www.dropbox.com/s/m0alzz7m9c6t88u/fashion-mnist-sprite.png?dl=1\" width=\"600\">\n",
    "\n",
    "There are 10 classes. Each training and testing example is assigned to one of the following labels:\n",
    "\n",
    "| Label | Description |\n",
    "| --- | --- |\n",
    "| 0 | T-shirt/top |\n",
    "| 1 | Trouser |\n",
    "| 2 | Pullover |\n",
    "| 3 | Dress |\n",
    "| 4 | Coat |\n",
    "| 5 | Sandal |\n",
    "| 6 | Shirt |\n",
    "| 7 | Sneaker |\n",
    "| 8 | Bag |\n",
    "| 9 | Ankle boot |\n",
    "\n",
    "\n",
    "We want to perform image classification on this dataset using the LeNet network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt  # one of the best graphics library for python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "from typing import Iterable\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font style=\"color:blue\">1. LeNet Architecture with BatchNorm</font>\n",
    "We have already explained the architecture for LeNet in the previous notebook.\n",
    "\n",
    "We will create another model called LeNetBN where we add Batch Normalization layers to the 2 conv blocks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # convolution layers\n",
    "        self._body = nn.Sequential(\n",
    "            # First convolution Layer\n",
    "            # input size = (32, 32), output size = (28, 28)\n",
    "            nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5),\n",
    "            # ReLU activation\n",
    "            nn.ReLU(inplace=True),\n",
    "            # Max pool 2-d\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            \n",
    "            # Second convolution layer\n",
    "            # input size = (14, 14), output size = (10, 10)\n",
    "            nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            # output size = (5, 5)\n",
    "        )\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self._head = nn.Sequential(\n",
    "            # First fully connected layer\n",
    "            # in_features = total number of weights in last conv layer = 16 * 5 * 5\n",
    "            nn.Linear(in_features=16 * 5 * 5, out_features=120), \n",
    "            \n",
    "            # ReLU activation\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # second fully connected layer\n",
    "            # in_features = output of last linear layer = 120 \n",
    "            nn.Linear(in_features=120, out_features=84), \n",
    "            \n",
    "            # ReLU activation\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # Third fully connected layer which is also output layer\n",
    "            # in_features = output of last linear layer = 84\n",
    "            # and out_features = number of classes = 10 (MNIST data 0-9)\n",
    "            nn.Linear(in_features=84, out_features=10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # apply feature extractor\n",
    "        x = self._body(x)\n",
    "        # flatten the output of conv layers\n",
    "        # dimension should be batch_size * number_of weight_in_last conv_layer\n",
    "        x = x.view(x.size()[0], -1)\n",
    "        # apply classification head\n",
    "        x = self._head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNetBN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # convolution layers\n",
    "        self._body = nn.Sequential(\n",
    "            # First convolution Layer\n",
    "            # input size = (32, 32), output size = (28, 28)\n",
    "            nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5),\n",
    "            nn.BatchNorm2d(6),\n",
    "            # ReLU activation\n",
    "            nn.ReLU(inplace=True),\n",
    "            # Max pool 2-d\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            \n",
    "            # Second convolution layer\n",
    "            # input size = (14, 14), output size = (10, 10)\n",
    "            nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            # output size = (5, 5)\n",
    "        )\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self._head = nn.Sequential(\n",
    "            # First fully connected layer\n",
    "            # in_features = total number of weight in last conv layer = 16 * 5 * 5\n",
    "            nn.Linear(in_features=16 * 5 * 5, out_features=120), \n",
    "            \n",
    "            # ReLU activation\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # second fully connected layer\n",
    "            # in_features = output of last linear layer = 120 \n",
    "            nn.Linear(in_features=120, out_features=84), \n",
    "            \n",
    "            # ReLU activation\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # Third fully connected layer. It is also output layer\n",
    "            # in_features = output of last linear layer = 84\n",
    "            # and out_features = number of classes = 10 (MNIST data 0-9)\n",
    "            nn.Linear(in_features=84, out_features=10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # apply feature extractor\n",
    "        x = self._body(x)\n",
    "        # flatten the output of conv layers\n",
    "        # dimension should be batch_size * number_of weights_in_last conv_layer\n",
    "        x = x.view(x.size()[0], -1)\n",
    "        # apply classification head\n",
    "        x = self._head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font style=\"color:blue\">2. Display the Network</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeNet(\n",
      "  (_body): Sequential(\n",
      "    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (_head): Sequential(\n",
      "    (0): Linear(in_features=400, out_features=120, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Linear(in_features=120, out_features=84, bias=True)\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): Linear(in_features=84, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "LeNetBN(\n",
      "  (_body): Sequential(\n",
      "    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (_head): Sequential(\n",
      "    (0): Linear(in_features=400, out_features=120, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Linear(in_features=120, out_features=84, bias=True)\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): Linear(in_features=84, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "lenet_model = LeNet()\n",
    "print(lenet_model)\n",
    "lenetBN_model = LeNetBN()\n",
    "print(lenetBN_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "# <font style=\"color:blue\">3. Get MNIST Data</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(batch_size, data_root='data', num_workers=1):\n",
    "    \n",
    "    train_test_transforms = transforms.Compose([\n",
    "        # Resize to 32X32\n",
    "        transforms.Resize((32, 32)),\n",
    "        # this re-scales image tensor values between 0-1. image_tensor /= 255\n",
    "        transforms.ToTensor(),\n",
    "        # subtract mean (0.2860) and divide by variance (0.3530).\n",
    "        # This mean and variance is calculated on training data (verify for yourself)\n",
    "        transforms.Normalize((0.2860, ), (0.3530, ))\n",
    "    ])\n",
    "    \n",
    "    # train dataloader\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.FashionMNIST(root=data_root, train=True, download=True, transform=train_test_transforms),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "    \n",
    "    # test dataloader\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.FashionMNIST(root=data_root, train=False, download=True, transform=train_test_transforms),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font style=\"color:blue\">4. System Configuration</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SystemConfiguration:\n",
    "    '''\n",
    "    Describes the common system setting needed for reproducible training\n",
    "    '''\n",
    "    seed: int = 42  # seed number to set the state of all random number generators\n",
    "    cudnn_benchmark_enabled: bool = True  # enable CuDNN benchmark for the sake of performance\n",
    "    cudnn_deterministic: bool = True  # make cudnn deterministic (reproducible training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font style=\"color:blue\">5. Training Configuration</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainingConfiguration:\n",
    "    '''\n",
    "    Describes configuration of the training process\n",
    "    '''\n",
    "    batch_size: int = 32  # amount of data to pass through the network at each forward-backward iteration\n",
    "    epochs_count: int = 20  # number of times the whole dataset will be passed through the network\n",
    "    learning_rate: float = 0.01  # determines the speed of network's weights update\n",
    "    log_interval: int = 100  # how many batches to wait between logging training status\n",
    "    test_interval: int = 1  # how many epochs to wait before another test. Set to 1 to get val loss at each epoch\n",
    "    data_root: str = \"data\"  # folder to save MNIST data (default: data)\n",
    "    num_workers: int = 10  # number of concurrent processes used to prepare data\n",
    "    device: str = 'cuda'  # device to use for training.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font style=\"color:blue\">6. System Setup</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_system(system_config: SystemConfiguration) -> None:\n",
    "    torch.manual_seed(system_config.seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.backends.cudnn_benchmark_enabled = system_config.cudnn_benchmark_enabled\n",
    "        torch.backends.cudnn.deterministic = system_config.cudnn_deterministic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font style=\"color:blue\">7. Training</font>\n",
    "We are familiar with the training pipeline used in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    train_config: TrainingConfiguration, model: nn.Module, optimizer: torch.optim.Optimizer,\n",
    "    train_loader: torch.utils.data.DataLoader, epoch_idx: int\n",
    ") -> None:\n",
    "    \n",
    "    # change model in training mode\n",
    "    model.train()\n",
    "    \n",
    "    # to get batch loss\n",
    "    batch_loss = np.array([])\n",
    "    \n",
    "    # to get batch accuracy\n",
    "    batch_acc = np.array([])\n",
    "        \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        \n",
    "        # clone target\n",
    "        indx_target = target.clone()\n",
    "        # send data to device (its is medatory if GPU has to be used)\n",
    "        data = data.to(train_config.device)\n",
    "        # send target to device\n",
    "        target = target.to(train_config.device)\n",
    "\n",
    "        # reset parameters gradient to zero\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward pass to the model\n",
    "        output = model(data)\n",
    "        \n",
    "        # cross entropy loss\n",
    "        loss = F.cross_entropy(output, target)\n",
    "        \n",
    "        # find gradients w.r.t training parameters\n",
    "        loss.backward()\n",
    "        # Update parameters using gradients\n",
    "        optimizer.step()\n",
    "        \n",
    "        batch_loss = np.append(batch_loss, [loss.item()])\n",
    "        \n",
    "        # get probability score using softmax\n",
    "        prob = F.softmax(output, dim=1)\n",
    "            \n",
    "        # get the index of the max probability\n",
    "        pred = prob.data.max(dim=1)[1]  \n",
    "                        \n",
    "        # correct prediction\n",
    "        correct = pred.cpu().eq(indx_target).sum()\n",
    "            \n",
    "        # accuracy\n",
    "        acc = float(correct) / float(len(data))\n",
    "        \n",
    "        batch_acc = np.append(batch_acc, [acc])\n",
    "\n",
    "        if batch_idx % train_config.log_interval == 0 and batch_idx > 0:              \n",
    "            print(\n",
    "                'Train Epoch: {} [{}/{}] Loss: {:.6f} Acc: {:.4f}'.format(\n",
    "                    epoch_idx, batch_idx * len(data), len(train_loader.dataset), loss.item(), acc\n",
    "                )\n",
    "            )\n",
    "            \n",
    "    epoch_loss = batch_loss.mean()\n",
    "    epoch_acc = batch_acc.mean()\n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font style=\"color:blue\">8. Validation</font>\n",
    "\n",
    "After every few epochs **`validation`** will be called with the `trained model` and `test_loader` to get validation loss and accuracy.\n",
    "\n",
    "**NOTE** that we are using `model.eval()` to enable evaluation mode of the model. This will turn off the running estimate calculation of mean and variance of data and just use the mean and variance computed while training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(\n",
    "    train_config: TrainingConfiguration,\n",
    "    model: nn.Module,\n",
    "    test_loader: torch.utils.data.DataLoader,\n",
    ") -> float:\n",
    "    # \n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    count_corect_predictions = 0\n",
    "    for data, target in test_loader:\n",
    "        indx_target = target.clone()\n",
    "        data = data.to(train_config.device)\n",
    "        \n",
    "        target = target.to(train_config.device)\n",
    "        \n",
    "        output = model(data)\n",
    "        # add loss for each mini batch\n",
    "        test_loss += F.cross_entropy(output, target).item()\n",
    "        \n",
    "        # get probability score using softmax\n",
    "        prob = F.softmax(output, dim=1)\n",
    "        \n",
    "        # get the index of the max probability\n",
    "        pred = prob.data.max(dim=1)[1] \n",
    "        \n",
    "        # add correct prediction count\n",
    "        count_corect_predictions += pred.cpu().eq(indx_target).sum()\n",
    "\n",
    "    # average over number of mini-batches\n",
    "    test_loss = test_loss / len(test_loader)  \n",
    "    \n",
    "    # average over number of dataset\n",
    "    accuracy = 100. * count_corect_predictions / len(test_loader.dataset)\n",
    "    \n",
    "    print(\n",
    "        '\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "            test_loss, count_corect_predictions, len(test_loader.dataset), accuracy\n",
    "        )\n",
    "    )\n",
    "    return test_loss, accuracy/100.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font style=\"color:blue\">9. Main</font>\n",
    "\n",
    "In this section of code, we use the configuration parameters defined above and start the training. Here are the important actions being taken in the code below:\n",
    "\n",
    "1. Set up system parameters like CPU/GPU, number of threads etc.\n",
    "1. Load the data using dataloaders.\n",
    "1. Create an instance of the LeNet model.\n",
    "1. Specify optimizer to use.\n",
    "1. Set up variables to track loss and accuracy and start training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(model, system_configuration=SystemConfiguration(), training_configuration=TrainingConfiguration()):\n",
    "    \n",
    "    # system configuration\n",
    "    setup_system(system_configuration)\n",
    "\n",
    "    # batch size\n",
    "    batch_size_to_set = training_configuration.batch_size\n",
    "    # num_workers\n",
    "    num_workers_to_set = training_configuration.num_workers\n",
    "    # epochs\n",
    "    epoch_num_to_set = training_configuration.epochs_count\n",
    "\n",
    "    # if GPU is available use training config, \n",
    "    # else lower batch_size, num_workers and epochs count\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda\"\n",
    "    else:\n",
    "        device = \"cpu\"\n",
    "        batch_size_to_set = 16\n",
    "        num_workers_to_set = 2\n",
    "        epoch_num_to_set = 10\n",
    "\n",
    "    # data loader\n",
    "    train_loader, test_loader = get_data(\n",
    "        batch_size=batch_size_to_set,\n",
    "        data_root=training_configuration.data_root,\n",
    "        num_workers=num_workers_to_set\n",
    "    )\n",
    "    \n",
    "    # Update training configuration\n",
    "    training_configuration = TrainingConfiguration(\n",
    "        device=device,\n",
    "        epochs_count=epoch_num_to_set,\n",
    "        batch_size=batch_size_to_set,\n",
    "        num_workers=num_workers_to_set\n",
    "    )\n",
    "        \n",
    "    # send model to device (GPU/CPU)\n",
    "    model.to(training_configuration.device)\n",
    "\n",
    "    # optimizer\n",
    "    optimizer = optim.SGD(\n",
    "        model.parameters(),\n",
    "        lr=training_configuration.learning_rate\n",
    "    )\n",
    "\n",
    "    best_loss = torch.tensor(np.inf)\n",
    "    \n",
    "    # epoch train/test loss\n",
    "    epoch_train_loss = np.array([])\n",
    "    epoch_test_loss = np.array([])\n",
    "    \n",
    "    # epch train/test accuracy\n",
    "    epoch_train_acc = np.array([])\n",
    "    epoch_test_acc = np.array([])\n",
    "    \n",
    "    # trainig time measurement\n",
    "    t_begin = time.time()\n",
    "    for epoch in range(training_configuration.epochs_count):\n",
    "        \n",
    "        train_loss, train_acc = train(training_configuration, model, optimizer, train_loader, epoch)\n",
    "        \n",
    "        epoch_train_loss = np.append(epoch_train_loss, [train_loss])\n",
    "        \n",
    "        epoch_train_acc = np.append(epoch_train_acc, [train_acc])\n",
    "\n",
    "        elapsed_time = time.time() - t_begin\n",
    "        speed_epoch = elapsed_time / (epoch + 1)\n",
    "        speed_batch = speed_epoch / len(train_loader)\n",
    "        eta = speed_epoch * training_configuration.epochs_count - elapsed_time\n",
    "        \n",
    "        print(\n",
    "            \"Elapsed {:.2f}s, {:.2f} s/epoch, {:.2f} s/batch, ets {:.2f}s\".format(\n",
    "                elapsed_time, speed_epoch, speed_batch, eta\n",
    "            )\n",
    "        )\n",
    "\n",
    "        if epoch % training_configuration.test_interval == 0:\n",
    "            current_loss, current_accuracy = validate(training_configuration, model, test_loader)\n",
    "            \n",
    "            epoch_test_loss = np.append(epoch_test_loss, [current_loss])\n",
    "        \n",
    "            epoch_test_acc = np.append(epoch_test_acc, [current_accuracy])\n",
    "            \n",
    "            if current_loss < best_loss:\n",
    "                best_loss = current_loss\n",
    "                \n",
    "    print(\"Total time: {:.2f}, Best Loss: {:.3f}\".format(time.time() - t_begin, best_loss))\n",
    "    \n",
    "    return model, epoch_train_loss, epoch_train_acc, epoch_test_loss, epoch_test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data\\FashionMNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71782298810b4f96ad3944e7f9535721",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\FashionMNIST\\raw\\train-images-idx3-ubyte.gz to data\\FashionMNIST\\raw\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data\\FashionMNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb5ea2d01b95496ca55664082b8e916c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\FashionMNIST\\raw\\train-labels-idx1-ubyte.gz to data\\FashionMNIST\\raw\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data\\FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ca6a8e48ee44e99a60f5dbec596ec58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz to data\\FashionMNIST\\raw\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data\\FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5da45fbeab649f6b1d5f1d63fb0dd25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data\\FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz to data\\FashionMNIST\\raw\n",
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\merri\\anaconda3\\lib\\site-packages\\torchvision\\datasets\\mnist.py:480: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:141.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [3200/60000] Loss: 2.281200 Acc: 0.2188\n",
      "Train Epoch: 0 [6400/60000] Loss: 2.244358 Acc: 0.3125\n",
      "Train Epoch: 0 [9600/60000] Loss: 1.720222 Acc: 0.4688\n",
      "Train Epoch: 0 [12800/60000] Loss: 1.101108 Acc: 0.6250\n",
      "Train Epoch: 0 [16000/60000] Loss: 1.248917 Acc: 0.5625\n",
      "Train Epoch: 0 [19200/60000] Loss: 0.743523 Acc: 0.6562\n",
      "Train Epoch: 0 [22400/60000] Loss: 0.676694 Acc: 0.8125\n",
      "Train Epoch: 0 [25600/60000] Loss: 0.780536 Acc: 0.6875\n",
      "Train Epoch: 0 [28800/60000] Loss: 1.272776 Acc: 0.4688\n",
      "Train Epoch: 0 [32000/60000] Loss: 0.828402 Acc: 0.6562\n",
      "Train Epoch: 0 [35200/60000] Loss: 0.830930 Acc: 0.6562\n",
      "Train Epoch: 0 [38400/60000] Loss: 0.771851 Acc: 0.7500\n",
      "Train Epoch: 0 [41600/60000] Loss: 0.451245 Acc: 0.7812\n",
      "Train Epoch: 0 [44800/60000] Loss: 0.845278 Acc: 0.7188\n",
      "Train Epoch: 0 [48000/60000] Loss: 0.716907 Acc: 0.6562\n",
      "Train Epoch: 0 [51200/60000] Loss: 0.653296 Acc: 0.7500\n",
      "Train Epoch: 0 [54400/60000] Loss: 0.642304 Acc: 0.7188\n",
      "Train Epoch: 0 [57600/60000] Loss: 0.692387 Acc: 0.7500\n",
      "Elapsed 10.51s, 10.51 s/epoch, 0.01 s/batch, ets 199.77s\n",
      "\n",
      "Test set: Average loss: 0.6631, Accuracy: 7507/10000 (75%)\n",
      "\n",
      "Train Epoch: 1 [3200/60000] Loss: 0.727250 Acc: 0.7188\n",
      "Train Epoch: 1 [6400/60000] Loss: 0.517861 Acc: 0.8438\n",
      "Train Epoch: 1 [9600/60000] Loss: 0.556850 Acc: 0.7812\n",
      "Train Epoch: 1 [12800/60000] Loss: 0.650513 Acc: 0.7500\n",
      "Train Epoch: 1 [16000/60000] Loss: 0.577841 Acc: 0.7812\n",
      "Train Epoch: 1 [19200/60000] Loss: 0.451667 Acc: 0.8125\n",
      "Train Epoch: 1 [22400/60000] Loss: 0.982125 Acc: 0.6875\n",
      "Train Epoch: 1 [25600/60000] Loss: 0.348015 Acc: 0.8750\n",
      "Train Epoch: 1 [28800/60000] Loss: 0.477886 Acc: 0.8438\n",
      "Train Epoch: 1 [32000/60000] Loss: 0.483751 Acc: 0.9062\n",
      "Train Epoch: 1 [35200/60000] Loss: 0.743789 Acc: 0.7500\n",
      "Train Epoch: 1 [38400/60000] Loss: 0.744808 Acc: 0.6562\n",
      "Train Epoch: 1 [41600/60000] Loss: 0.386471 Acc: 0.8438\n",
      "Train Epoch: 1 [44800/60000] Loss: 0.630653 Acc: 0.7500\n",
      "Train Epoch: 1 [48000/60000] Loss: 0.740037 Acc: 0.6562\n",
      "Train Epoch: 1 [51200/60000] Loss: 0.464409 Acc: 0.8125\n",
      "Train Epoch: 1 [54400/60000] Loss: 0.636844 Acc: 0.7500\n",
      "Train Epoch: 1 [57600/60000] Loss: 0.647431 Acc: 0.7500\n",
      "Elapsed 23.48s, 11.74 s/epoch, 0.01 s/batch, ets 211.33s\n",
      "\n",
      "Test set: Average loss: 0.5115, Accuracy: 8109/10000 (81%)\n",
      "\n",
      "Train Epoch: 2 [3200/60000] Loss: 0.767701 Acc: 0.6875\n",
      "Train Epoch: 2 [6400/60000] Loss: 0.377136 Acc: 0.7812\n",
      "Train Epoch: 2 [9600/60000] Loss: 0.549349 Acc: 0.7500\n",
      "Train Epoch: 2 [12800/60000] Loss: 0.472628 Acc: 0.7812\n",
      "Train Epoch: 2 [16000/60000] Loss: 0.681634 Acc: 0.8438\n",
      "Train Epoch: 2 [19200/60000] Loss: 0.405300 Acc: 0.7812\n",
      "Train Epoch: 2 [22400/60000] Loss: 0.345185 Acc: 0.8125\n",
      "Train Epoch: 2 [25600/60000] Loss: 0.364369 Acc: 0.8750\n",
      "Train Epoch: 2 [28800/60000] Loss: 0.247832 Acc: 0.9062\n",
      "Train Epoch: 2 [32000/60000] Loss: 0.361242 Acc: 0.8750\n",
      "Train Epoch: 2 [35200/60000] Loss: 0.564900 Acc: 0.7812\n",
      "Train Epoch: 2 [38400/60000] Loss: 0.523970 Acc: 0.8125\n",
      "Train Epoch: 2 [41600/60000] Loss: 0.375521 Acc: 0.8125\n",
      "Train Epoch: 2 [44800/60000] Loss: 0.256892 Acc: 0.9062\n",
      "Train Epoch: 2 [48000/60000] Loss: 0.356148 Acc: 0.9062\n",
      "Train Epoch: 2 [51200/60000] Loss: 0.724157 Acc: 0.8125\n",
      "Train Epoch: 2 [54400/60000] Loss: 0.476913 Acc: 0.8438\n",
      "Train Epoch: 2 [57600/60000] Loss: 0.398118 Acc: 0.8438\n",
      "Elapsed 36.72s, 12.24 s/epoch, 0.01 s/batch, ets 208.06s\n",
      "\n",
      "Test set: Average loss: 0.4630, Accuracy: 8295/10000 (83%)\n",
      "\n",
      "Train Epoch: 3 [3200/60000] Loss: 0.558419 Acc: 0.7500\n",
      "Train Epoch: 3 [6400/60000] Loss: 0.440919 Acc: 0.8125\n",
      "Train Epoch: 3 [9600/60000] Loss: 0.722961 Acc: 0.6875\n",
      "Train Epoch: 3 [12800/60000] Loss: 0.512398 Acc: 0.8438\n",
      "Train Epoch: 3 [16000/60000] Loss: 0.345707 Acc: 0.9062\n",
      "Train Epoch: 3 [19200/60000] Loss: 0.405403 Acc: 0.8125\n",
      "Train Epoch: 3 [22400/60000] Loss: 0.314649 Acc: 0.9375\n",
      "Train Epoch: 3 [25600/60000] Loss: 0.352365 Acc: 0.8750\n",
      "Train Epoch: 3 [28800/60000] Loss: 0.326089 Acc: 0.9062\n",
      "Train Epoch: 3 [32000/60000] Loss: 0.541065 Acc: 0.8125\n",
      "Train Epoch: 3 [35200/60000] Loss: 0.363587 Acc: 0.8750\n",
      "Train Epoch: 3 [38400/60000] Loss: 0.281447 Acc: 0.9375\n",
      "Train Epoch: 3 [41600/60000] Loss: 0.144191 Acc: 0.9375\n",
      "Train Epoch: 3 [44800/60000] Loss: 0.306221 Acc: 0.9688\n",
      "Train Epoch: 3 [48000/60000] Loss: 0.599110 Acc: 0.8125\n",
      "Train Epoch: 3 [51200/60000] Loss: 0.304749 Acc: 0.8750\n",
      "Train Epoch: 3 [54400/60000] Loss: 0.467574 Acc: 0.8125\n",
      "Train Epoch: 3 [57600/60000] Loss: 0.313144 Acc: 0.8750\n",
      "Elapsed 50.07s, 12.52 s/epoch, 0.01 s/batch, ets 200.29s\n",
      "\n",
      "Test set: Average loss: 0.4127, Accuracy: 8506/10000 (85%)\n",
      "\n",
      "Train Epoch: 4 [3200/60000] Loss: 0.561120 Acc: 0.8125\n",
      "Train Epoch: 4 [6400/60000] Loss: 0.622212 Acc: 0.8125\n",
      "Train Epoch: 4 [9600/60000] Loss: 0.238237 Acc: 0.9375\n",
      "Train Epoch: 4 [12800/60000] Loss: 0.186979 Acc: 0.9062\n",
      "Train Epoch: 4 [16000/60000] Loss: 0.452519 Acc: 0.9062\n",
      "Train Epoch: 4 [19200/60000] Loss: 0.286876 Acc: 0.9375\n",
      "Train Epoch: 4 [22400/60000] Loss: 0.344010 Acc: 0.8750\n",
      "Train Epoch: 4 [25600/60000] Loss: 0.471848 Acc: 0.7812\n",
      "Train Epoch: 4 [28800/60000] Loss: 0.229788 Acc: 0.9062\n",
      "Train Epoch: 4 [32000/60000] Loss: 0.417727 Acc: 0.7812\n",
      "Train Epoch: 4 [35200/60000] Loss: 0.345295 Acc: 0.8125\n",
      "Train Epoch: 4 [38400/60000] Loss: 0.104750 Acc: 1.0000\n",
      "Train Epoch: 4 [41600/60000] Loss: 0.334234 Acc: 0.8438\n",
      "Train Epoch: 4 [44800/60000] Loss: 0.318478 Acc: 0.8750\n",
      "Train Epoch: 4 [48000/60000] Loss: 0.368636 Acc: 0.8750\n",
      "Train Epoch: 4 [51200/60000] Loss: 0.152642 Acc: 0.9688\n",
      "Train Epoch: 4 [54400/60000] Loss: 0.163833 Acc: 1.0000\n",
      "Train Epoch: 4 [57600/60000] Loss: 0.278082 Acc: 0.9062\n",
      "Elapsed 63.21s, 12.64 s/epoch, 0.01 s/batch, ets 189.63s\n",
      "\n",
      "Test set: Average loss: 0.3894, Accuracy: 8542/10000 (85%)\n",
      "\n",
      "Train Epoch: 5 [3200/60000] Loss: 0.375255 Acc: 0.8750\n",
      "Train Epoch: 5 [6400/60000] Loss: 0.400266 Acc: 0.8750\n",
      "Train Epoch: 5 [9600/60000] Loss: 0.371711 Acc: 0.8125\n",
      "Train Epoch: 5 [12800/60000] Loss: 0.241192 Acc: 0.9062\n",
      "Train Epoch: 5 [16000/60000] Loss: 0.497273 Acc: 0.7812\n",
      "Train Epoch: 5 [19200/60000] Loss: 0.443263 Acc: 0.8438\n",
      "Train Epoch: 5 [22400/60000] Loss: 0.260497 Acc: 0.9062\n",
      "Train Epoch: 5 [25600/60000] Loss: 0.233016 Acc: 0.9688\n",
      "Train Epoch: 5 [28800/60000] Loss: 0.376065 Acc: 0.7812\n",
      "Train Epoch: 5 [32000/60000] Loss: 0.347373 Acc: 0.8438\n",
      "Train Epoch: 5 [35200/60000] Loss: 0.286438 Acc: 0.9062\n",
      "Train Epoch: 5 [38400/60000] Loss: 0.389182 Acc: 0.8438\n",
      "Train Epoch: 5 [41600/60000] Loss: 0.287561 Acc: 0.8750\n",
      "Train Epoch: 5 [44800/60000] Loss: 0.233021 Acc: 0.9062\n",
      "Train Epoch: 5 [48000/60000] Loss: 0.368809 Acc: 0.7812\n",
      "Train Epoch: 5 [51200/60000] Loss: 0.247078 Acc: 0.8125\n",
      "Train Epoch: 5 [54400/60000] Loss: 0.232558 Acc: 0.9062\n",
      "Train Epoch: 5 [57600/60000] Loss: 0.262490 Acc: 0.9062\n",
      "Elapsed 76.30s, 12.72 s/epoch, 0.01 s/batch, ets 178.04s\n",
      "\n",
      "Test set: Average loss: 0.3764, Accuracy: 8629/10000 (86%)\n",
      "\n",
      "Train Epoch: 6 [3200/60000] Loss: 0.386591 Acc: 0.9062\n",
      "Train Epoch: 6 [6400/60000] Loss: 0.280006 Acc: 0.9062\n",
      "Train Epoch: 6 [9600/60000] Loss: 0.362635 Acc: 0.8438\n",
      "Train Epoch: 6 [12800/60000] Loss: 0.262781 Acc: 0.9375\n",
      "Train Epoch: 6 [16000/60000] Loss: 0.327558 Acc: 0.8438\n",
      "Train Epoch: 6 [19200/60000] Loss: 0.453141 Acc: 0.8125\n",
      "Train Epoch: 6 [22400/60000] Loss: 0.462502 Acc: 0.8125\n",
      "Train Epoch: 6 [25600/60000] Loss: 0.145299 Acc: 0.9688\n",
      "Train Epoch: 6 [28800/60000] Loss: 0.361931 Acc: 0.8125\n",
      "Train Epoch: 6 [32000/60000] Loss: 0.397305 Acc: 0.7812\n",
      "Train Epoch: 6 [35200/60000] Loss: 0.239614 Acc: 0.8750\n",
      "Train Epoch: 6 [38400/60000] Loss: 0.232809 Acc: 0.9062\n",
      "Train Epoch: 6 [41600/60000] Loss: 0.453547 Acc: 0.8438\n",
      "Train Epoch: 6 [44800/60000] Loss: 0.293146 Acc: 0.9062\n",
      "Train Epoch: 6 [48000/60000] Loss: 0.492396 Acc: 0.7500\n",
      "Train Epoch: 6 [51200/60000] Loss: 0.461830 Acc: 0.8438\n",
      "Train Epoch: 6 [54400/60000] Loss: 0.400167 Acc: 0.8438\n",
      "Train Epoch: 6 [57600/60000] Loss: 0.234255 Acc: 0.8438\n",
      "Elapsed 89.46s, 12.78 s/epoch, 0.01 s/batch, ets 166.14s\n",
      "\n",
      "Test set: Average loss: 0.3870, Accuracy: 8540/10000 (85%)\n",
      "\n",
      "Train Epoch: 7 [3200/60000] Loss: 0.233450 Acc: 0.8750\n",
      "Train Epoch: 7 [6400/60000] Loss: 0.451334 Acc: 0.7812\n",
      "Train Epoch: 7 [9600/60000] Loss: 0.427750 Acc: 0.8750\n",
      "Train Epoch: 7 [12800/60000] Loss: 0.234179 Acc: 0.9375\n",
      "Train Epoch: 7 [16000/60000] Loss: 0.605936 Acc: 0.7188\n",
      "Train Epoch: 7 [19200/60000] Loss: 0.272068 Acc: 0.9375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 7 [22400/60000] Loss: 0.172768 Acc: 0.9375\n",
      "Train Epoch: 7 [25600/60000] Loss: 0.761457 Acc: 0.7500\n",
      "Train Epoch: 7 [28800/60000] Loss: 0.419892 Acc: 0.7500\n",
      "Train Epoch: 7 [32000/60000] Loss: 0.346722 Acc: 0.8125\n",
      "Train Epoch: 7 [35200/60000] Loss: 0.189283 Acc: 0.9062\n",
      "Train Epoch: 7 [38400/60000] Loss: 0.253904 Acc: 0.8750\n",
      "Train Epoch: 7 [41600/60000] Loss: 0.171316 Acc: 0.9062\n",
      "Train Epoch: 7 [44800/60000] Loss: 0.327098 Acc: 0.8125\n",
      "Train Epoch: 7 [48000/60000] Loss: 0.387742 Acc: 0.8125\n",
      "Train Epoch: 7 [51200/60000] Loss: 0.335957 Acc: 0.9375\n",
      "Train Epoch: 7 [54400/60000] Loss: 0.453728 Acc: 0.8125\n",
      "Train Epoch: 7 [57600/60000] Loss: 0.458076 Acc: 0.8438\n",
      "Elapsed 102.53s, 12.82 s/epoch, 0.01 s/batch, ets 153.80s\n",
      "\n",
      "Test set: Average loss: 0.3659, Accuracy: 8678/10000 (87%)\n",
      "\n",
      "Train Epoch: 8 [3200/60000] Loss: 0.255026 Acc: 0.9375\n",
      "Train Epoch: 8 [6400/60000] Loss: 0.287728 Acc: 0.8750\n",
      "Train Epoch: 8 [9600/60000] Loss: 0.217709 Acc: 0.9062\n",
      "Train Epoch: 8 [12800/60000] Loss: 0.210155 Acc: 0.9688\n",
      "Train Epoch: 8 [16000/60000] Loss: 0.393443 Acc: 0.8125\n",
      "Train Epoch: 8 [19200/60000] Loss: 0.364824 Acc: 0.8750\n",
      "Train Epoch: 8 [22400/60000] Loss: 0.278316 Acc: 0.8750\n",
      "Train Epoch: 8 [25600/60000] Loss: 0.221071 Acc: 0.9062\n",
      "Train Epoch: 8 [28800/60000] Loss: 0.463186 Acc: 0.8125\n",
      "Train Epoch: 8 [32000/60000] Loss: 0.167711 Acc: 0.9375\n",
      "Train Epoch: 8 [35200/60000] Loss: 0.498825 Acc: 0.7812\n",
      "Train Epoch: 8 [38400/60000] Loss: 0.403460 Acc: 0.8125\n",
      "Train Epoch: 8 [41600/60000] Loss: 0.202621 Acc: 0.9062\n",
      "Train Epoch: 8 [44800/60000] Loss: 0.161310 Acc: 0.9688\n",
      "Train Epoch: 8 [48000/60000] Loss: 0.271326 Acc: 0.9062\n",
      "Train Epoch: 8 [51200/60000] Loss: 0.353762 Acc: 0.9062\n",
      "Train Epoch: 8 [54400/60000] Loss: 0.228017 Acc: 0.9062\n",
      "Train Epoch: 8 [57600/60000] Loss: 0.234561 Acc: 0.8750\n",
      "Elapsed 115.55s, 12.84 s/epoch, 0.01 s/batch, ets 141.23s\n",
      "\n",
      "Test set: Average loss: 0.3450, Accuracy: 8743/10000 (87%)\n",
      "\n",
      "Train Epoch: 9 [3200/60000] Loss: 0.236168 Acc: 0.9062\n",
      "Train Epoch: 9 [6400/60000] Loss: 0.174570 Acc: 0.9688\n",
      "Train Epoch: 9 [9600/60000] Loss: 0.256012 Acc: 0.9062\n",
      "Train Epoch: 9 [12800/60000] Loss: 0.543685 Acc: 0.8438\n",
      "Train Epoch: 9 [16000/60000] Loss: 0.127845 Acc: 0.9688\n",
      "Train Epoch: 9 [19200/60000] Loss: 0.365228 Acc: 0.8438\n",
      "Train Epoch: 9 [22400/60000] Loss: 0.277842 Acc: 0.9062\n",
      "Train Epoch: 9 [25600/60000] Loss: 0.310602 Acc: 0.8438\n",
      "Train Epoch: 9 [28800/60000] Loss: 0.223857 Acc: 0.9062\n",
      "Train Epoch: 9 [32000/60000] Loss: 0.178377 Acc: 0.9375\n",
      "Train Epoch: 9 [35200/60000] Loss: 0.369413 Acc: 0.9062\n",
      "Train Epoch: 9 [38400/60000] Loss: 0.304807 Acc: 0.8438\n",
      "Train Epoch: 9 [41600/60000] Loss: 0.213517 Acc: 0.9375\n",
      "Train Epoch: 9 [44800/60000] Loss: 0.537285 Acc: 0.7500\n",
      "Train Epoch: 9 [48000/60000] Loss: 0.440728 Acc: 0.8125\n",
      "Train Epoch: 9 [51200/60000] Loss: 0.111221 Acc: 0.9688\n",
      "Train Epoch: 9 [54400/60000] Loss: 0.316025 Acc: 0.9688\n",
      "Train Epoch: 9 [57600/60000] Loss: 0.244071 Acc: 0.9375\n",
      "Elapsed 128.67s, 12.87 s/epoch, 0.01 s/batch, ets 128.67s\n",
      "\n",
      "Test set: Average loss: 0.3311, Accuracy: 8791/10000 (88%)\n",
      "\n",
      "Train Epoch: 10 [3200/60000] Loss: 0.232061 Acc: 0.8750\n",
      "Train Epoch: 10 [6400/60000] Loss: 0.132566 Acc: 0.9375\n",
      "Train Epoch: 10 [9600/60000] Loss: 0.225154 Acc: 0.9375\n",
      "Train Epoch: 10 [12800/60000] Loss: 0.350225 Acc: 0.8125\n",
      "Train Epoch: 10 [16000/60000] Loss: 0.460416 Acc: 0.7812\n",
      "Train Epoch: 10 [19200/60000] Loss: 0.367427 Acc: 0.8438\n",
      "Train Epoch: 10 [22400/60000] Loss: 0.265757 Acc: 0.8750\n",
      "Train Epoch: 10 [25600/60000] Loss: 0.407688 Acc: 0.9375\n",
      "Train Epoch: 10 [28800/60000] Loss: 0.254849 Acc: 0.9062\n",
      "Train Epoch: 10 [32000/60000] Loss: 0.255860 Acc: 0.9062\n",
      "Train Epoch: 10 [35200/60000] Loss: 0.078637 Acc: 0.9688\n",
      "Train Epoch: 10 [38400/60000] Loss: 0.156766 Acc: 0.9688\n",
      "Train Epoch: 10 [41600/60000] Loss: 0.148847 Acc: 0.9688\n",
      "Train Epoch: 10 [44800/60000] Loss: 0.276874 Acc: 0.8125\n",
      "Train Epoch: 10 [48000/60000] Loss: 0.460919 Acc: 0.8750\n",
      "Train Epoch: 10 [51200/60000] Loss: 0.337564 Acc: 0.8125\n",
      "Train Epoch: 10 [54400/60000] Loss: 0.401055 Acc: 0.9062\n",
      "Train Epoch: 10 [57600/60000] Loss: 0.115325 Acc: 0.9688\n",
      "Elapsed 141.86s, 12.90 s/epoch, 0.01 s/batch, ets 116.06s\n",
      "\n",
      "Test set: Average loss: 0.3274, Accuracy: 8815/10000 (88%)\n",
      "\n",
      "Train Epoch: 11 [3200/60000] Loss: 0.207832 Acc: 0.9375\n",
      "Train Epoch: 11 [6400/60000] Loss: 0.124474 Acc: 1.0000\n",
      "Train Epoch: 11 [9600/60000] Loss: 0.194351 Acc: 0.9375\n",
      "Train Epoch: 11 [12800/60000] Loss: 0.270820 Acc: 0.9062\n",
      "Train Epoch: 11 [16000/60000] Loss: 0.344362 Acc: 0.9062\n",
      "Train Epoch: 11 [19200/60000] Loss: 0.075736 Acc: 1.0000\n",
      "Train Epoch: 11 [22400/60000] Loss: 0.337464 Acc: 0.8125\n",
      "Train Epoch: 11 [25600/60000] Loss: 0.259513 Acc: 0.9062\n",
      "Train Epoch: 11 [28800/60000] Loss: 0.251417 Acc: 0.8750\n",
      "Train Epoch: 11 [32000/60000] Loss: 0.495930 Acc: 0.8438\n",
      "Train Epoch: 11 [35200/60000] Loss: 0.204381 Acc: 0.9375\n",
      "Train Epoch: 11 [38400/60000] Loss: 0.169337 Acc: 0.9688\n",
      "Train Epoch: 11 [41600/60000] Loss: 0.137337 Acc: 0.9375\n",
      "Train Epoch: 11 [44800/60000] Loss: 0.246533 Acc: 0.9062\n",
      "Train Epoch: 11 [48000/60000] Loss: 0.130251 Acc: 0.9688\n",
      "Train Epoch: 11 [51200/60000] Loss: 0.380780 Acc: 0.8750\n",
      "Train Epoch: 11 [54400/60000] Loss: 0.376041 Acc: 0.9375\n",
      "Train Epoch: 11 [57600/60000] Loss: 0.308215 Acc: 0.9062\n",
      "Elapsed 154.96s, 12.91 s/epoch, 0.01 s/batch, ets 103.31s\n",
      "\n",
      "Test set: Average loss: 0.3139, Accuracy: 8854/10000 (89%)\n",
      "\n",
      "Train Epoch: 12 [3200/60000] Loss: 0.229503 Acc: 0.9062\n",
      "Train Epoch: 12 [6400/60000] Loss: 0.203479 Acc: 0.9375\n",
      "Train Epoch: 12 [9600/60000] Loss: 0.243684 Acc: 0.8750\n",
      "Train Epoch: 12 [12800/60000] Loss: 0.275272 Acc: 0.8750\n",
      "Train Epoch: 12 [16000/60000] Loss: 0.201811 Acc: 0.9062\n",
      "Train Epoch: 12 [19200/60000] Loss: 0.530457 Acc: 0.8438\n",
      "Train Epoch: 12 [22400/60000] Loss: 0.234673 Acc: 0.9375\n",
      "Train Epoch: 12 [25600/60000] Loss: 0.334172 Acc: 0.8438\n",
      "Train Epoch: 12 [28800/60000] Loss: 0.321470 Acc: 0.8750\n",
      "Train Epoch: 12 [32000/60000] Loss: 0.224499 Acc: 0.9688\n",
      "Train Epoch: 12 [35200/60000] Loss: 0.116948 Acc: 0.9688\n",
      "Train Epoch: 12 [38400/60000] Loss: 0.359929 Acc: 0.8438\n",
      "Train Epoch: 12 [41600/60000] Loss: 0.260020 Acc: 0.9688\n",
      "Train Epoch: 12 [44800/60000] Loss: 0.195260 Acc: 0.9062\n",
      "Train Epoch: 12 [48000/60000] Loss: 0.227405 Acc: 0.8750\n",
      "Train Epoch: 12 [51200/60000] Loss: 0.382667 Acc: 0.8125\n",
      "Train Epoch: 12 [54400/60000] Loss: 0.232460 Acc: 0.8750\n",
      "Train Epoch: 12 [57600/60000] Loss: 0.205139 Acc: 0.9375\n",
      "Elapsed 167.98s, 12.92 s/epoch, 0.01 s/batch, ets 90.45s\n",
      "\n",
      "Test set: Average loss: 0.3178, Accuracy: 8828/10000 (88%)\n",
      "\n",
      "Train Epoch: 13 [3200/60000] Loss: 0.220241 Acc: 0.9062\n",
      "Train Epoch: 13 [6400/60000] Loss: 0.365856 Acc: 0.9062\n",
      "Train Epoch: 13 [9600/60000] Loss: 0.406474 Acc: 0.8438\n",
      "Train Epoch: 13 [12800/60000] Loss: 0.373778 Acc: 0.8125\n",
      "Train Epoch: 13 [16000/60000] Loss: 0.247813 Acc: 0.9062\n",
      "Train Epoch: 13 [19200/60000] Loss: 0.253983 Acc: 0.9375\n",
      "Train Epoch: 13 [22400/60000] Loss: 0.257912 Acc: 0.8750\n",
      "Train Epoch: 13 [25600/60000] Loss: 0.407034 Acc: 0.8438\n",
      "Train Epoch: 13 [28800/60000] Loss: 0.165438 Acc: 0.9688\n",
      "Train Epoch: 13 [32000/60000] Loss: 0.189616 Acc: 0.9375\n",
      "Train Epoch: 13 [35200/60000] Loss: 0.355244 Acc: 0.8750\n",
      "Train Epoch: 13 [38400/60000] Loss: 0.352840 Acc: 0.7812\n",
      "Train Epoch: 13 [41600/60000] Loss: 0.351721 Acc: 0.8125\n",
      "Train Epoch: 13 [44800/60000] Loss: 0.135124 Acc: 0.9375\n",
      "Train Epoch: 13 [48000/60000] Loss: 0.214444 Acc: 0.9375\n",
      "Train Epoch: 13 [51200/60000] Loss: 0.231515 Acc: 0.9062\n",
      "Train Epoch: 13 [54400/60000] Loss: 0.281966 Acc: 0.8125\n",
      "Train Epoch: 13 [57600/60000] Loss: 0.176346 Acc: 0.9375\n",
      "Elapsed 181.03s, 12.93 s/epoch, 0.01 s/batch, ets 77.59s\n",
      "\n",
      "Test set: Average loss: 0.3028, Accuracy: 8907/10000 (89%)\n",
      "\n",
      "Train Epoch: 14 [3200/60000] Loss: 0.252765 Acc: 0.8750\n",
      "Train Epoch: 14 [6400/60000] Loss: 0.231983 Acc: 0.9375\n",
      "Train Epoch: 14 [9600/60000] Loss: 0.238994 Acc: 0.9062\n",
      "Train Epoch: 14 [12800/60000] Loss: 0.092292 Acc: 1.0000\n",
      "Train Epoch: 14 [16000/60000] Loss: 0.250478 Acc: 0.9375\n",
      "Train Epoch: 14 [19200/60000] Loss: 0.352601 Acc: 0.9375\n",
      "Train Epoch: 14 [22400/60000] Loss: 0.334928 Acc: 0.8125\n",
      "Train Epoch: 14 [25600/60000] Loss: 0.298174 Acc: 0.8750\n",
      "Train Epoch: 14 [28800/60000] Loss: 0.206934 Acc: 0.8750\n",
      "Train Epoch: 14 [32000/60000] Loss: 0.218317 Acc: 0.8750\n",
      "Train Epoch: 14 [35200/60000] Loss: 0.370003 Acc: 0.8125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 14 [38400/60000] Loss: 0.248884 Acc: 0.9062\n",
      "Train Epoch: 14 [41600/60000] Loss: 0.222408 Acc: 0.9375\n",
      "Train Epoch: 14 [44800/60000] Loss: 0.273650 Acc: 0.8750\n",
      "Train Epoch: 14 [48000/60000] Loss: 0.289400 Acc: 0.9062\n",
      "Train Epoch: 14 [51200/60000] Loss: 0.338736 Acc: 0.9062\n",
      "Train Epoch: 14 [54400/60000] Loss: 0.081675 Acc: 1.0000\n",
      "Train Epoch: 14 [57600/60000] Loss: 0.226793 Acc: 0.9375\n",
      "Elapsed 194.07s, 12.94 s/epoch, 0.01 s/batch, ets 64.69s\n",
      "\n",
      "Test set: Average loss: 0.3241, Accuracy: 8803/10000 (88%)\n",
      "\n",
      "Train Epoch: 15 [3200/60000] Loss: 0.267011 Acc: 0.8438\n",
      "Train Epoch: 15 [6400/60000] Loss: 0.132371 Acc: 0.9375\n",
      "Train Epoch: 15 [9600/60000] Loss: 0.342800 Acc: 0.8750\n",
      "Train Epoch: 15 [12800/60000] Loss: 0.143768 Acc: 0.9688\n",
      "Train Epoch: 15 [16000/60000] Loss: 0.359476 Acc: 0.8750\n",
      "Train Epoch: 15 [19200/60000] Loss: 0.301157 Acc: 0.8750\n",
      "Train Epoch: 15 [22400/60000] Loss: 0.532524 Acc: 0.8438\n",
      "Train Epoch: 15 [25600/60000] Loss: 0.074826 Acc: 1.0000\n",
      "Train Epoch: 15 [28800/60000] Loss: 0.290281 Acc: 0.8438\n",
      "Train Epoch: 15 [32000/60000] Loss: 0.163437 Acc: 0.9062\n",
      "Train Epoch: 15 [35200/60000] Loss: 0.135301 Acc: 0.9375\n",
      "Train Epoch: 15 [38400/60000] Loss: 0.292127 Acc: 0.8750\n",
      "Train Epoch: 15 [41600/60000] Loss: 0.178921 Acc: 0.9688\n",
      "Train Epoch: 15 [44800/60000] Loss: 0.223029 Acc: 0.9375\n",
      "Train Epoch: 15 [48000/60000] Loss: 0.277487 Acc: 0.8438\n",
      "Train Epoch: 15 [51200/60000] Loss: 0.436512 Acc: 0.7812\n",
      "Train Epoch: 15 [54400/60000] Loss: 0.226486 Acc: 0.9062\n",
      "Train Epoch: 15 [57600/60000] Loss: 0.288407 Acc: 0.9375\n",
      "Elapsed 207.12s, 12.94 s/epoch, 0.01 s/batch, ets 51.78s\n",
      "\n",
      "Test set: Average loss: 0.2989, Accuracy: 8895/10000 (89%)\n",
      "\n",
      "Train Epoch: 16 [3200/60000] Loss: 0.074882 Acc: 1.0000\n",
      "Train Epoch: 16 [6400/60000] Loss: 0.159973 Acc: 0.9688\n",
      "Train Epoch: 16 [9600/60000] Loss: 0.286275 Acc: 0.8750\n",
      "Train Epoch: 16 [12800/60000] Loss: 0.125062 Acc: 0.9375\n",
      "Train Epoch: 16 [16000/60000] Loss: 0.225404 Acc: 0.9062\n",
      "Train Epoch: 16 [19200/60000] Loss: 0.167923 Acc: 0.9062\n",
      "Train Epoch: 16 [22400/60000] Loss: 0.144391 Acc: 0.9688\n",
      "Train Epoch: 16 [25600/60000] Loss: 0.241331 Acc: 0.9062\n",
      "Train Epoch: 16 [28800/60000] Loss: 0.308514 Acc: 0.9062\n",
      "Train Epoch: 16 [32000/60000] Loss: 0.188018 Acc: 0.9688\n",
      "Train Epoch: 16 [35200/60000] Loss: 0.211218 Acc: 0.9062\n",
      "Train Epoch: 16 [38400/60000] Loss: 0.161867 Acc: 0.9062\n",
      "Train Epoch: 16 [41600/60000] Loss: 0.142147 Acc: 0.9688\n",
      "Train Epoch: 16 [44800/60000] Loss: 0.205054 Acc: 0.9062\n",
      "Train Epoch: 16 [48000/60000] Loss: 0.175449 Acc: 0.9375\n",
      "Train Epoch: 16 [51200/60000] Loss: 0.125055 Acc: 0.9375\n",
      "Train Epoch: 16 [54400/60000] Loss: 0.090004 Acc: 0.9688\n",
      "Train Epoch: 16 [57600/60000] Loss: 0.179457 Acc: 0.9375\n",
      "Elapsed 220.15s, 12.95 s/epoch, 0.01 s/batch, ets 38.85s\n",
      "\n",
      "Test set: Average loss: 0.3231, Accuracy: 8812/10000 (88%)\n",
      "\n",
      "Train Epoch: 17 [3200/60000] Loss: 0.202366 Acc: 0.9375\n",
      "Train Epoch: 17 [6400/60000] Loss: 0.198623 Acc: 0.9062\n",
      "Train Epoch: 17 [9600/60000] Loss: 0.305651 Acc: 0.9062\n",
      "Train Epoch: 17 [12800/60000] Loss: 0.276590 Acc: 0.9375\n",
      "Train Epoch: 17 [16000/60000] Loss: 0.227332 Acc: 0.8750\n",
      "Train Epoch: 17 [19200/60000] Loss: 0.172504 Acc: 0.9062\n",
      "Train Epoch: 17 [22400/60000] Loss: 0.333768 Acc: 0.8750\n",
      "Train Epoch: 17 [25600/60000] Loss: 0.111549 Acc: 1.0000\n",
      "Train Epoch: 17 [28800/60000] Loss: 0.205358 Acc: 0.9688\n",
      "Train Epoch: 17 [32000/60000] Loss: 0.140540 Acc: 0.9375\n",
      "Train Epoch: 17 [35200/60000] Loss: 0.310833 Acc: 0.8750\n",
      "Train Epoch: 17 [38400/60000] Loss: 0.157803 Acc: 0.9375\n",
      "Train Epoch: 17 [41600/60000] Loss: 0.231074 Acc: 0.9062\n",
      "Train Epoch: 17 [44800/60000] Loss: 0.056799 Acc: 1.0000\n",
      "Train Epoch: 17 [48000/60000] Loss: 0.159250 Acc: 0.9375\n",
      "Train Epoch: 17 [51200/60000] Loss: 0.070658 Acc: 0.9688\n",
      "Train Epoch: 17 [54400/60000] Loss: 0.360351 Acc: 0.8750\n",
      "Train Epoch: 17 [57600/60000] Loss: 0.097736 Acc: 0.9688\n",
      "Elapsed 233.27s, 12.96 s/epoch, 0.01 s/batch, ets 25.92s\n",
      "\n",
      "Test set: Average loss: 0.3006, Accuracy: 8919/10000 (89%)\n",
      "\n",
      "Train Epoch: 18 [3200/60000] Loss: 0.110545 Acc: 0.9375\n",
      "Train Epoch: 18 [6400/60000] Loss: 0.332452 Acc: 0.8125\n",
      "Train Epoch: 18 [9600/60000] Loss: 0.126276 Acc: 0.9688\n",
      "Train Epoch: 18 [12800/60000] Loss: 0.228238 Acc: 0.9062\n",
      "Train Epoch: 18 [16000/60000] Loss: 0.243639 Acc: 0.9375\n",
      "Train Epoch: 18 [19200/60000] Loss: 0.365664 Acc: 0.8750\n",
      "Train Epoch: 18 [22400/60000] Loss: 0.128250 Acc: 0.9375\n",
      "Train Epoch: 18 [25600/60000] Loss: 0.227468 Acc: 0.9062\n",
      "Train Epoch: 18 [28800/60000] Loss: 0.538193 Acc: 0.8438\n",
      "Train Epoch: 18 [32000/60000] Loss: 0.102347 Acc: 0.9375\n",
      "Train Epoch: 18 [35200/60000] Loss: 0.390896 Acc: 0.8438\n",
      "Train Epoch: 18 [38400/60000] Loss: 0.190374 Acc: 0.9062\n",
      "Train Epoch: 18 [41600/60000] Loss: 0.166164 Acc: 0.9688\n",
      "Train Epoch: 18 [44800/60000] Loss: 0.170498 Acc: 0.9688\n",
      "Train Epoch: 18 [48000/60000] Loss: 0.259074 Acc: 0.9375\n",
      "Train Epoch: 18 [51200/60000] Loss: 0.228072 Acc: 0.9062\n",
      "Train Epoch: 18 [54400/60000] Loss: 0.233182 Acc: 0.9375\n",
      "Train Epoch: 18 [57600/60000] Loss: 0.187446 Acc: 0.9062\n",
      "Elapsed 246.37s, 12.97 s/epoch, 0.01 s/batch, ets 12.97s\n",
      "\n",
      "Test set: Average loss: 0.2901, Accuracy: 8970/10000 (90%)\n",
      "\n",
      "Train Epoch: 19 [3200/60000] Loss: 0.235801 Acc: 0.9062\n",
      "Train Epoch: 19 [6400/60000] Loss: 0.260363 Acc: 0.9062\n",
      "Train Epoch: 19 [9600/60000] Loss: 0.224522 Acc: 0.9688\n",
      "Train Epoch: 19 [12800/60000] Loss: 0.472189 Acc: 0.8438\n",
      "Train Epoch: 19 [16000/60000] Loss: 0.090450 Acc: 0.9688\n",
      "Train Epoch: 19 [19200/60000] Loss: 0.248059 Acc: 0.9062\n",
      "Train Epoch: 19 [22400/60000] Loss: 0.115609 Acc: 0.9375\n",
      "Train Epoch: 19 [25600/60000] Loss: 0.276026 Acc: 0.8750\n",
      "Train Epoch: 19 [28800/60000] Loss: 0.314087 Acc: 0.8750\n",
      "Train Epoch: 19 [32000/60000] Loss: 0.366096 Acc: 0.8438\n",
      "Train Epoch: 19 [35200/60000] Loss: 0.193896 Acc: 0.8750\n",
      "Train Epoch: 19 [38400/60000] Loss: 0.133771 Acc: 0.9375\n",
      "Train Epoch: 19 [41600/60000] Loss: 0.289530 Acc: 0.8750\n",
      "Train Epoch: 19 [44800/60000] Loss: 0.214801 Acc: 0.9062\n",
      "Train Epoch: 19 [48000/60000] Loss: 0.298278 Acc: 0.9062\n",
      "Train Epoch: 19 [51200/60000] Loss: 0.263168 Acc: 0.9062\n",
      "Train Epoch: 19 [54400/60000] Loss: 0.236258 Acc: 0.9375\n",
      "Train Epoch: 19 [57600/60000] Loss: 0.209391 Acc: 0.9062\n",
      "Elapsed 259.65s, 12.98 s/epoch, 0.01 s/batch, ets 0.00s\n",
      "\n",
      "Test set: Average loss: 0.2945, Accuracy: 8920/10000 (89%)\n",
      "\n",
      "Total time: 263.07, Best Loss: 0.290\n",
      "Train Epoch: 0 [3200/60000] Loss: 1.991647 Acc: 0.3438\n",
      "Train Epoch: 0 [6400/60000] Loss: 1.403383 Acc: 0.5938\n",
      "Train Epoch: 0 [9600/60000] Loss: 0.813885 Acc: 0.8125\n",
      "Train Epoch: 0 [12800/60000] Loss: 0.780840 Acc: 0.7188\n",
      "Train Epoch: 0 [16000/60000] Loss: 0.761504 Acc: 0.6562\n",
      "Train Epoch: 0 [19200/60000] Loss: 0.514472 Acc: 0.8125\n",
      "Train Epoch: 0 [22400/60000] Loss: 0.468185 Acc: 0.8750\n",
      "Train Epoch: 0 [25600/60000] Loss: 0.442831 Acc: 0.8438\n",
      "Train Epoch: 0 [28800/60000] Loss: 0.882218 Acc: 0.6562\n",
      "Train Epoch: 0 [32000/60000] Loss: 0.674415 Acc: 0.6875\n",
      "Train Epoch: 0 [35200/60000] Loss: 0.539572 Acc: 0.8750\n",
      "Train Epoch: 0 [38400/60000] Loss: 0.611597 Acc: 0.7812\n",
      "Train Epoch: 0 [41600/60000] Loss: 0.329871 Acc: 0.9062\n",
      "Train Epoch: 0 [44800/60000] Loss: 0.648684 Acc: 0.6875\n",
      "Train Epoch: 0 [48000/60000] Loss: 0.527054 Acc: 0.8125\n",
      "Train Epoch: 0 [51200/60000] Loss: 0.453736 Acc: 0.7812\n",
      "Train Epoch: 0 [54400/60000] Loss: 0.438919 Acc: 0.8438\n",
      "Train Epoch: 0 [57600/60000] Loss: 0.464679 Acc: 0.8438\n",
      "Elapsed 10.49s, 10.49 s/epoch, 0.01 s/batch, ets 199.32s\n",
      "\n",
      "Test set: Average loss: 0.4966, Accuracy: 8115/10000 (81%)\n",
      "\n",
      "Train Epoch: 1 [3200/60000] Loss: 0.440458 Acc: 0.8438\n",
      "Train Epoch: 1 [6400/60000] Loss: 0.273260 Acc: 0.9375\n",
      "Train Epoch: 1 [9600/60000] Loss: 0.595886 Acc: 0.6562\n",
      "Train Epoch: 1 [12800/60000] Loss: 0.278424 Acc: 0.8750\n",
      "Train Epoch: 1 [16000/60000] Loss: 0.345012 Acc: 0.9062\n",
      "Train Epoch: 1 [19200/60000] Loss: 0.256051 Acc: 0.9062\n",
      "Train Epoch: 1 [22400/60000] Loss: 0.757351 Acc: 0.8125\n",
      "Train Epoch: 1 [25600/60000] Loss: 0.255947 Acc: 0.9062\n",
      "Train Epoch: 1 [28800/60000] Loss: 0.358192 Acc: 0.8750\n",
      "Train Epoch: 1 [32000/60000] Loss: 0.310762 Acc: 0.8750\n",
      "Train Epoch: 1 [35200/60000] Loss: 0.373982 Acc: 0.8750\n",
      "Train Epoch: 1 [38400/60000] Loss: 0.511809 Acc: 0.8125\n",
      "Train Epoch: 1 [41600/60000] Loss: 0.310756 Acc: 0.8750\n",
      "Train Epoch: 1 [44800/60000] Loss: 0.327278 Acc: 0.8750\n",
      "Train Epoch: 1 [48000/60000] Loss: 0.541453 Acc: 0.8438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [51200/60000] Loss: 0.453941 Acc: 0.8125\n",
      "Train Epoch: 1 [54400/60000] Loss: 0.363495 Acc: 0.8750\n",
      "Train Epoch: 1 [57600/60000] Loss: 0.529615 Acc: 0.8125\n",
      "Elapsed 24.35s, 12.18 s/epoch, 0.01 s/batch, ets 219.19s\n",
      "\n",
      "Test set: Average loss: 0.4114, Accuracy: 8504/10000 (85%)\n",
      "\n",
      "Train Epoch: 2 [3200/60000] Loss: 0.530146 Acc: 0.8438\n",
      "Train Epoch: 2 [6400/60000] Loss: 0.252531 Acc: 0.9062\n",
      "Train Epoch: 2 [9600/60000] Loss: 0.278055 Acc: 0.9375\n",
      "Train Epoch: 2 [12800/60000] Loss: 0.249530 Acc: 0.9375\n",
      "Train Epoch: 2 [16000/60000] Loss: 0.531333 Acc: 0.8438\n",
      "Train Epoch: 2 [19200/60000] Loss: 0.351696 Acc: 0.8125\n",
      "Train Epoch: 2 [22400/60000] Loss: 0.215588 Acc: 0.9062\n",
      "Train Epoch: 2 [25600/60000] Loss: 0.291091 Acc: 0.9062\n",
      "Train Epoch: 2 [28800/60000] Loss: 0.189660 Acc: 0.9375\n",
      "Train Epoch: 2 [32000/60000] Loss: 0.208314 Acc: 0.9062\n",
      "Train Epoch: 2 [35200/60000] Loss: 0.494912 Acc: 0.8438\n",
      "Train Epoch: 2 [38400/60000] Loss: 0.331242 Acc: 0.8750\n",
      "Train Epoch: 2 [41600/60000] Loss: 0.342470 Acc: 0.9062\n",
      "Train Epoch: 2 [44800/60000] Loss: 0.204040 Acc: 0.9375\n",
      "Train Epoch: 2 [48000/60000] Loss: 0.253916 Acc: 0.9062\n",
      "Train Epoch: 2 [51200/60000] Loss: 0.529685 Acc: 0.8438\n",
      "Train Epoch: 2 [54400/60000] Loss: 0.421235 Acc: 0.8438\n",
      "Train Epoch: 2 [57600/60000] Loss: 0.271229 Acc: 0.8750\n",
      "Elapsed 39.10s, 13.03 s/epoch, 0.01 s/batch, ets 221.59s\n",
      "\n",
      "Test set: Average loss: 0.3869, Accuracy: 8597/10000 (86%)\n",
      "\n",
      "Train Epoch: 3 [3200/60000] Loss: 0.490355 Acc: 0.8438\n",
      "Train Epoch: 3 [6400/60000] Loss: 0.344114 Acc: 0.8750\n",
      "Train Epoch: 3 [9600/60000] Loss: 0.493484 Acc: 0.7812\n",
      "Train Epoch: 3 [12800/60000] Loss: 0.303450 Acc: 0.9062\n",
      "Train Epoch: 3 [16000/60000] Loss: 0.257612 Acc: 0.9062\n",
      "Train Epoch: 3 [19200/60000] Loss: 0.285747 Acc: 0.8750\n",
      "Train Epoch: 3 [22400/60000] Loss: 0.175578 Acc: 0.9688\n",
      "Train Epoch: 3 [25600/60000] Loss: 0.328253 Acc: 0.9062\n",
      "Train Epoch: 3 [28800/60000] Loss: 0.171156 Acc: 0.9375\n",
      "Train Epoch: 3 [32000/60000] Loss: 0.417537 Acc: 0.8125\n",
      "Train Epoch: 3 [35200/60000] Loss: 0.298423 Acc: 0.8438\n",
      "Train Epoch: 3 [38400/60000] Loss: 0.141180 Acc: 0.9375\n",
      "Train Epoch: 3 [41600/60000] Loss: 0.143699 Acc: 0.9375\n",
      "Train Epoch: 3 [44800/60000] Loss: 0.280287 Acc: 0.8750\n",
      "Train Epoch: 3 [48000/60000] Loss: 0.511384 Acc: 0.8438\n",
      "Train Epoch: 3 [51200/60000] Loss: 0.163369 Acc: 0.9375\n",
      "Train Epoch: 3 [54400/60000] Loss: 0.440466 Acc: 0.8125\n",
      "Train Epoch: 3 [57600/60000] Loss: 0.331254 Acc: 0.8750\n",
      "Elapsed 53.08s, 13.27 s/epoch, 0.01 s/batch, ets 212.32s\n",
      "\n",
      "Test set: Average loss: 0.3390, Accuracy: 8807/10000 (88%)\n",
      "\n",
      "Train Epoch: 4 [3200/60000] Loss: 0.254248 Acc: 0.9375\n",
      "Train Epoch: 4 [6400/60000] Loss: 0.398537 Acc: 0.8125\n",
      "Train Epoch: 4 [9600/60000] Loss: 0.201027 Acc: 0.9062\n",
      "Train Epoch: 4 [12800/60000] Loss: 0.071598 Acc: 0.9688\n",
      "Train Epoch: 4 [16000/60000] Loss: 0.219556 Acc: 0.9375\n",
      "Train Epoch: 4 [19200/60000] Loss: 0.220718 Acc: 0.8438\n",
      "Train Epoch: 4 [22400/60000] Loss: 0.173173 Acc: 0.9062\n",
      "Train Epoch: 4 [25600/60000] Loss: 0.334818 Acc: 0.8438\n",
      "Train Epoch: 4 [28800/60000] Loss: 0.088580 Acc: 1.0000\n",
      "Train Epoch: 4 [32000/60000] Loss: 0.228672 Acc: 0.9062\n",
      "Train Epoch: 4 [35200/60000] Loss: 0.258739 Acc: 0.9062\n",
      "Train Epoch: 4 [38400/60000] Loss: 0.121343 Acc: 0.9375\n",
      "Train Epoch: 4 [41600/60000] Loss: 0.335304 Acc: 0.8438\n",
      "Train Epoch: 4 [44800/60000] Loss: 0.255058 Acc: 0.9062\n",
      "Train Epoch: 4 [48000/60000] Loss: 0.300144 Acc: 0.8125\n",
      "Train Epoch: 4 [51200/60000] Loss: 0.135737 Acc: 0.9688\n",
      "Train Epoch: 4 [54400/60000] Loss: 0.078815 Acc: 1.0000\n",
      "Train Epoch: 4 [57600/60000] Loss: 0.234806 Acc: 0.9062\n",
      "Elapsed 67.23s, 13.45 s/epoch, 0.01 s/batch, ets 201.70s\n",
      "\n",
      "Test set: Average loss: 0.3317, Accuracy: 8801/10000 (88%)\n",
      "\n",
      "Train Epoch: 5 [3200/60000] Loss: 0.293479 Acc: 0.8750\n",
      "Train Epoch: 5 [6400/60000] Loss: 0.448125 Acc: 0.8438\n",
      "Train Epoch: 5 [9600/60000] Loss: 0.170994 Acc: 0.9688\n",
      "Train Epoch: 5 [12800/60000] Loss: 0.222608 Acc: 0.9375\n",
      "Train Epoch: 5 [16000/60000] Loss: 0.409242 Acc: 0.9062\n",
      "Train Epoch: 5 [19200/60000] Loss: 0.406362 Acc: 0.9062\n",
      "Train Epoch: 5 [22400/60000] Loss: 0.181299 Acc: 0.9062\n",
      "Train Epoch: 5 [25600/60000] Loss: 0.301877 Acc: 0.9375\n",
      "Train Epoch: 5 [28800/60000] Loss: 0.271300 Acc: 0.8750\n",
      "Train Epoch: 5 [32000/60000] Loss: 0.214512 Acc: 0.9062\n",
      "Train Epoch: 5 [35200/60000] Loss: 0.225384 Acc: 0.9375\n",
      "Train Epoch: 5 [38400/60000] Loss: 0.419992 Acc: 0.8438\n",
      "Train Epoch: 5 [41600/60000] Loss: 0.240110 Acc: 0.9062\n",
      "Train Epoch: 5 [44800/60000] Loss: 0.199472 Acc: 0.9062\n",
      "Train Epoch: 5 [48000/60000] Loss: 0.323255 Acc: 0.8438\n",
      "Train Epoch: 5 [51200/60000] Loss: 0.119528 Acc: 1.0000\n",
      "Train Epoch: 5 [54400/60000] Loss: 0.107946 Acc: 0.9688\n",
      "Train Epoch: 5 [57600/60000] Loss: 0.167471 Acc: 0.9688\n",
      "Elapsed 81.29s, 13.55 s/epoch, 0.01 s/batch, ets 189.68s\n",
      "\n",
      "Test set: Average loss: 0.3335, Accuracy: 8821/10000 (88%)\n",
      "\n",
      "Train Epoch: 6 [3200/60000] Loss: 0.359382 Acc: 0.9375\n",
      "Train Epoch: 6 [6400/60000] Loss: 0.181976 Acc: 0.9688\n",
      "Train Epoch: 6 [9600/60000] Loss: 0.188275 Acc: 0.9375\n",
      "Train Epoch: 6 [12800/60000] Loss: 0.150793 Acc: 0.9375\n",
      "Train Epoch: 6 [16000/60000] Loss: 0.190150 Acc: 0.9375\n",
      "Train Epoch: 6 [19200/60000] Loss: 0.244314 Acc: 0.9062\n",
      "Train Epoch: 6 [22400/60000] Loss: 0.337431 Acc: 0.8438\n",
      "Train Epoch: 6 [25600/60000] Loss: 0.066829 Acc: 1.0000\n",
      "Train Epoch: 6 [28800/60000] Loss: 0.473255 Acc: 0.8438\n",
      "Train Epoch: 6 [32000/60000] Loss: 0.358452 Acc: 0.8750\n",
      "Train Epoch: 6 [35200/60000] Loss: 0.112532 Acc: 0.9688\n",
      "Train Epoch: 6 [38400/60000] Loss: 0.238126 Acc: 0.9375\n",
      "Train Epoch: 6 [41600/60000] Loss: 0.476419 Acc: 0.8438\n",
      "Train Epoch: 6 [44800/60000] Loss: 0.244524 Acc: 0.8750\n",
      "Train Epoch: 6 [48000/60000] Loss: 0.331666 Acc: 0.8438\n",
      "Train Epoch: 6 [51200/60000] Loss: 0.238432 Acc: 0.9375\n",
      "Train Epoch: 6 [54400/60000] Loss: 0.262036 Acc: 0.9062\n",
      "Train Epoch: 6 [57600/60000] Loss: 0.112077 Acc: 0.9375\n",
      "Elapsed 95.36s, 13.62 s/epoch, 0.01 s/batch, ets 177.09s\n",
      "\n",
      "Test set: Average loss: 0.3260, Accuracy: 8815/10000 (88%)\n",
      "\n",
      "Train Epoch: 7 [3200/60000] Loss: 0.140555 Acc: 0.9688\n",
      "Train Epoch: 7 [6400/60000] Loss: 0.251865 Acc: 0.9062\n",
      "Train Epoch: 7 [9600/60000] Loss: 0.283096 Acc: 0.9375\n",
      "Train Epoch: 7 [12800/60000] Loss: 0.219449 Acc: 0.8750\n",
      "Train Epoch: 7 [16000/60000] Loss: 0.517777 Acc: 0.8438\n",
      "Train Epoch: 7 [19200/60000] Loss: 0.302380 Acc: 0.9375\n",
      "Train Epoch: 7 [22400/60000] Loss: 0.208248 Acc: 0.9375\n",
      "Train Epoch: 7 [25600/60000] Loss: 0.693803 Acc: 0.7188\n",
      "Train Epoch: 7 [28800/60000] Loss: 0.327559 Acc: 0.8125\n",
      "Train Epoch: 7 [32000/60000] Loss: 0.331064 Acc: 0.8438\n",
      "Train Epoch: 7 [35200/60000] Loss: 0.246437 Acc: 0.8750\n",
      "Train Epoch: 7 [38400/60000] Loss: 0.130124 Acc: 0.9375\n",
      "Train Epoch: 7 [41600/60000] Loss: 0.126425 Acc: 0.9062\n",
      "Train Epoch: 7 [44800/60000] Loss: 0.180014 Acc: 0.9062\n",
      "Train Epoch: 7 [48000/60000] Loss: 0.387512 Acc: 0.7812\n",
      "Train Epoch: 7 [51200/60000] Loss: 0.316338 Acc: 0.9375\n",
      "Train Epoch: 7 [54400/60000] Loss: 0.435907 Acc: 0.9062\n",
      "Train Epoch: 7 [57600/60000] Loss: 0.402272 Acc: 0.8438\n",
      "Elapsed 109.66s, 13.71 s/epoch, 0.01 s/batch, ets 164.48s\n",
      "\n",
      "Test set: Average loss: 0.3049, Accuracy: 8898/10000 (89%)\n",
      "\n",
      "Train Epoch: 8 [3200/60000] Loss: 0.140599 Acc: 0.9375\n",
      "Train Epoch: 8 [6400/60000] Loss: 0.166098 Acc: 0.9375\n",
      "Train Epoch: 8 [9600/60000] Loss: 0.185331 Acc: 0.9062\n",
      "Train Epoch: 8 [12800/60000] Loss: 0.137677 Acc: 0.9688\n",
      "Train Epoch: 8 [16000/60000] Loss: 0.260126 Acc: 0.8750\n",
      "Train Epoch: 8 [19200/60000] Loss: 0.370847 Acc: 0.8438\n",
      "Train Epoch: 8 [22400/60000] Loss: 0.333084 Acc: 0.8438\n",
      "Train Epoch: 8 [25600/60000] Loss: 0.168213 Acc: 0.9688\n",
      "Train Epoch: 8 [28800/60000] Loss: 0.343136 Acc: 0.8438\n",
      "Train Epoch: 8 [32000/60000] Loss: 0.283420 Acc: 0.8750\n",
      "Train Epoch: 8 [35200/60000] Loss: 0.459325 Acc: 0.8750\n",
      "Train Epoch: 8 [38400/60000] Loss: 0.287692 Acc: 0.8750\n",
      "Train Epoch: 8 [41600/60000] Loss: 0.148660 Acc: 0.9688\n",
      "Train Epoch: 8 [44800/60000] Loss: 0.192010 Acc: 0.9688\n",
      "Train Epoch: 8 [48000/60000] Loss: 0.286562 Acc: 0.9062\n",
      "Train Epoch: 8 [51200/60000] Loss: 0.142839 Acc: 0.9375\n",
      "Train Epoch: 8 [54400/60000] Loss: 0.141142 Acc: 0.9375\n",
      "Train Epoch: 8 [57600/60000] Loss: 0.256520 Acc: 0.8750\n",
      "Elapsed 123.99s, 13.78 s/epoch, 0.01 s/batch, ets 151.54s\n",
      "\n",
      "Test set: Average loss: 0.3114, Accuracy: 8882/10000 (89%)\n",
      "\n",
      "Train Epoch: 9 [3200/60000] Loss: 0.197511 Acc: 0.9688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 9 [6400/60000] Loss: 0.144629 Acc: 0.9688\n",
      "Train Epoch: 9 [9600/60000] Loss: 0.119745 Acc: 0.9688\n",
      "Train Epoch: 9 [12800/60000] Loss: 0.521769 Acc: 0.8750\n",
      "Train Epoch: 9 [16000/60000] Loss: 0.078768 Acc: 1.0000\n",
      "Train Epoch: 9 [19200/60000] Loss: 0.196605 Acc: 0.9375\n",
      "Train Epoch: 9 [22400/60000] Loss: 0.336081 Acc: 0.8125\n",
      "Train Epoch: 9 [25600/60000] Loss: 0.304413 Acc: 0.8125\n",
      "Train Epoch: 9 [28800/60000] Loss: 0.159964 Acc: 0.9062\n",
      "Train Epoch: 9 [32000/60000] Loss: 0.074012 Acc: 0.9688\n",
      "Train Epoch: 9 [35200/60000] Loss: 0.540975 Acc: 0.8438\n",
      "Train Epoch: 9 [38400/60000] Loss: 0.241100 Acc: 0.8750\n",
      "Train Epoch: 9 [41600/60000] Loss: 0.105319 Acc: 0.9375\n",
      "Train Epoch: 9 [44800/60000] Loss: 0.635439 Acc: 0.7188\n",
      "Train Epoch: 9 [48000/60000] Loss: 0.260614 Acc: 0.9062\n",
      "Train Epoch: 9 [51200/60000] Loss: 0.067420 Acc: 1.0000\n",
      "Train Epoch: 9 [54400/60000] Loss: 0.167370 Acc: 0.9375\n",
      "Train Epoch: 9 [57600/60000] Loss: 0.192059 Acc: 0.9375\n",
      "Elapsed 138.41s, 13.84 s/epoch, 0.01 s/batch, ets 138.41s\n",
      "\n",
      "Test set: Average loss: 0.2984, Accuracy: 8919/10000 (89%)\n",
      "\n",
      "Train Epoch: 10 [3200/60000] Loss: 0.240291 Acc: 0.9375\n",
      "Train Epoch: 10 [6400/60000] Loss: 0.113022 Acc: 0.9688\n",
      "Train Epoch: 10 [9600/60000] Loss: 0.179100 Acc: 0.9375\n",
      "Train Epoch: 10 [12800/60000] Loss: 0.303249 Acc: 0.8750\n",
      "Train Epoch: 10 [16000/60000] Loss: 0.276689 Acc: 0.8750\n",
      "Train Epoch: 10 [19200/60000] Loss: 0.428966 Acc: 0.8125\n",
      "Train Epoch: 10 [22400/60000] Loss: 0.178262 Acc: 0.9375\n",
      "Train Epoch: 10 [25600/60000] Loss: 0.421937 Acc: 0.8750\n",
      "Train Epoch: 10 [28800/60000] Loss: 0.286897 Acc: 0.8438\n",
      "Train Epoch: 10 [32000/60000] Loss: 0.318639 Acc: 0.9062\n",
      "Train Epoch: 10 [35200/60000] Loss: 0.049277 Acc: 0.9688\n",
      "Train Epoch: 10 [38400/60000] Loss: 0.227099 Acc: 0.9062\n",
      "Train Epoch: 10 [41600/60000] Loss: 0.126539 Acc: 0.9375\n",
      "Train Epoch: 10 [44800/60000] Loss: 0.212771 Acc: 0.9062\n",
      "Train Epoch: 10 [48000/60000] Loss: 0.402186 Acc: 0.8750\n",
      "Train Epoch: 10 [51200/60000] Loss: 0.245965 Acc: 0.8750\n",
      "Train Epoch: 10 [54400/60000] Loss: 0.345030 Acc: 0.9062\n",
      "Train Epoch: 10 [57600/60000] Loss: 0.107263 Acc: 0.9688\n",
      "Elapsed 152.79s, 13.89 s/epoch, 0.01 s/batch, ets 125.01s\n",
      "\n",
      "Test set: Average loss: 0.3100, Accuracy: 8894/10000 (89%)\n",
      "\n",
      "Train Epoch: 11 [3200/60000] Loss: 0.146481 Acc: 0.9375\n",
      "Train Epoch: 11 [6400/60000] Loss: 0.134370 Acc: 0.9375\n",
      "Train Epoch: 11 [9600/60000] Loss: 0.091055 Acc: 0.9688\n",
      "Train Epoch: 11 [12800/60000] Loss: 0.378933 Acc: 0.8750\n",
      "Train Epoch: 11 [16000/60000] Loss: 0.115545 Acc: 0.9688\n",
      "Train Epoch: 11 [19200/60000] Loss: 0.059655 Acc: 1.0000\n",
      "Train Epoch: 11 [22400/60000] Loss: 0.300838 Acc: 0.7812\n",
      "Train Epoch: 11 [25600/60000] Loss: 0.299417 Acc: 0.8750\n",
      "Train Epoch: 11 [28800/60000] Loss: 0.162430 Acc: 0.9375\n",
      "Train Epoch: 11 [32000/60000] Loss: 0.421192 Acc: 0.8750\n",
      "Train Epoch: 11 [35200/60000] Loss: 0.101790 Acc: 0.9688\n",
      "Train Epoch: 11 [38400/60000] Loss: 0.130177 Acc: 0.9688\n",
      "Train Epoch: 11 [41600/60000] Loss: 0.179527 Acc: 0.9375\n",
      "Train Epoch: 11 [44800/60000] Loss: 0.144001 Acc: 0.9375\n",
      "Train Epoch: 11 [48000/60000] Loss: 0.170755 Acc: 0.9062\n",
      "Train Epoch: 11 [51200/60000] Loss: 0.302340 Acc: 0.9062\n",
      "Train Epoch: 11 [54400/60000] Loss: 0.326072 Acc: 0.9062\n",
      "Train Epoch: 11 [57600/60000] Loss: 0.208650 Acc: 0.9062\n",
      "Elapsed 167.14s, 13.93 s/epoch, 0.01 s/batch, ets 111.43s\n",
      "\n",
      "Test set: Average loss: 0.2885, Accuracy: 8959/10000 (90%)\n",
      "\n",
      "Train Epoch: 12 [3200/60000] Loss: 0.203166 Acc: 0.9375\n",
      "Train Epoch: 12 [6400/60000] Loss: 0.164201 Acc: 0.9375\n",
      "Train Epoch: 12 [9600/60000] Loss: 0.221673 Acc: 0.8750\n",
      "Train Epoch: 12 [12800/60000] Loss: 0.333788 Acc: 0.8750\n",
      "Train Epoch: 12 [16000/60000] Loss: 0.221947 Acc: 0.9062\n",
      "Train Epoch: 12 [19200/60000] Loss: 0.390866 Acc: 0.8750\n",
      "Train Epoch: 12 [22400/60000] Loss: 0.133157 Acc: 0.9375\n",
      "Train Epoch: 12 [25600/60000] Loss: 0.275168 Acc: 0.8750\n",
      "Train Epoch: 12 [28800/60000] Loss: 0.204184 Acc: 0.9062\n",
      "Train Epoch: 12 [32000/60000] Loss: 0.201547 Acc: 0.9375\n",
      "Train Epoch: 12 [35200/60000] Loss: 0.071797 Acc: 0.9688\n",
      "Train Epoch: 12 [38400/60000] Loss: 0.275522 Acc: 0.8750\n",
      "Train Epoch: 12 [41600/60000] Loss: 0.264315 Acc: 0.9688\n",
      "Train Epoch: 12 [44800/60000] Loss: 0.227562 Acc: 0.9375\n",
      "Train Epoch: 12 [48000/60000] Loss: 0.152081 Acc: 0.9375\n",
      "Train Epoch: 12 [51200/60000] Loss: 0.263411 Acc: 0.8750\n",
      "Train Epoch: 12 [54400/60000] Loss: 0.176876 Acc: 0.9062\n",
      "Train Epoch: 12 [57600/60000] Loss: 0.163279 Acc: 0.9062\n",
      "Elapsed 181.54s, 13.96 s/epoch, 0.01 s/batch, ets 97.75s\n",
      "\n",
      "Test set: Average loss: 0.2816, Accuracy: 8976/10000 (90%)\n",
      "\n",
      "Train Epoch: 13 [3200/60000] Loss: 0.187999 Acc: 0.9062\n",
      "Train Epoch: 13 [6400/60000] Loss: 0.313258 Acc: 0.9375\n",
      "Train Epoch: 13 [9600/60000] Loss: 0.313461 Acc: 0.8438\n",
      "Train Epoch: 13 [12800/60000] Loss: 0.355217 Acc: 0.8750\n",
      "Train Epoch: 13 [16000/60000] Loss: 0.172368 Acc: 0.9375\n",
      "Train Epoch: 13 [19200/60000] Loss: 0.193241 Acc: 0.9375\n",
      "Train Epoch: 13 [22400/60000] Loss: 0.173750 Acc: 0.9062\n",
      "Train Epoch: 13 [25600/60000] Loss: 0.329624 Acc: 0.8438\n",
      "Train Epoch: 13 [28800/60000] Loss: 0.108609 Acc: 0.9688\n",
      "Train Epoch: 13 [32000/60000] Loss: 0.110746 Acc: 0.9688\n",
      "Train Epoch: 13 [35200/60000] Loss: 0.246736 Acc: 0.9062\n",
      "Train Epoch: 13 [38400/60000] Loss: 0.214098 Acc: 0.9062\n",
      "Train Epoch: 13 [41600/60000] Loss: 0.217833 Acc: 0.9062\n",
      "Train Epoch: 13 [44800/60000] Loss: 0.181613 Acc: 0.9375\n",
      "Train Epoch: 13 [48000/60000] Loss: 0.134578 Acc: 0.9062\n",
      "Train Epoch: 13 [51200/60000] Loss: 0.190678 Acc: 0.9375\n",
      "Train Epoch: 13 [54400/60000] Loss: 0.244839 Acc: 0.9062\n",
      "Train Epoch: 13 [57600/60000] Loss: 0.212138 Acc: 0.9375\n",
      "Elapsed 195.89s, 13.99 s/epoch, 0.01 s/batch, ets 83.95s\n",
      "\n",
      "Test set: Average loss: 0.2896, Accuracy: 8945/10000 (89%)\n",
      "\n",
      "Train Epoch: 14 [3200/60000] Loss: 0.180976 Acc: 0.9375\n",
      "Train Epoch: 14 [6400/60000] Loss: 0.230418 Acc: 0.8750\n",
      "Train Epoch: 14 [9600/60000] Loss: 0.226069 Acc: 0.9375\n",
      "Train Epoch: 14 [12800/60000] Loss: 0.085546 Acc: 1.0000\n",
      "Train Epoch: 14 [16000/60000] Loss: 0.299698 Acc: 0.9062\n",
      "Train Epoch: 14 [19200/60000] Loss: 0.218180 Acc: 0.9688\n",
      "Train Epoch: 14 [22400/60000] Loss: 0.337857 Acc: 0.8438\n",
      "Train Epoch: 14 [25600/60000] Loss: 0.292543 Acc: 0.9375\n",
      "Train Epoch: 14 [28800/60000] Loss: 0.144569 Acc: 0.9062\n",
      "Train Epoch: 14 [32000/60000] Loss: 0.173033 Acc: 0.9375\n",
      "Train Epoch: 14 [35200/60000] Loss: 0.319751 Acc: 0.8750\n",
      "Train Epoch: 14 [38400/60000] Loss: 0.167933 Acc: 0.9375\n",
      "Train Epoch: 14 [41600/60000] Loss: 0.231936 Acc: 0.9062\n",
      "Train Epoch: 14 [44800/60000] Loss: 0.126132 Acc: 0.9688\n",
      "Train Epoch: 14 [48000/60000] Loss: 0.224959 Acc: 0.9375\n",
      "Train Epoch: 14 [51200/60000] Loss: 0.314860 Acc: 0.8438\n",
      "Train Epoch: 14 [54400/60000] Loss: 0.059209 Acc: 1.0000\n",
      "Train Epoch: 14 [57600/60000] Loss: 0.265919 Acc: 0.9062\n",
      "Elapsed 210.36s, 14.02 s/epoch, 0.01 s/batch, ets 70.12s\n",
      "\n",
      "Test set: Average loss: 0.2922, Accuracy: 8948/10000 (89%)\n",
      "\n",
      "Train Epoch: 15 [3200/60000] Loss: 0.133113 Acc: 0.9688\n",
      "Train Epoch: 15 [6400/60000] Loss: 0.117104 Acc: 0.9688\n",
      "Train Epoch: 15 [9600/60000] Loss: 0.192228 Acc: 0.9375\n",
      "Train Epoch: 15 [12800/60000] Loss: 0.120305 Acc: 0.9688\n",
      "Train Epoch: 15 [16000/60000] Loss: 0.287680 Acc: 0.9062\n",
      "Train Epoch: 15 [19200/60000] Loss: 0.148803 Acc: 0.9688\n",
      "Train Epoch: 15 [22400/60000] Loss: 0.517886 Acc: 0.8125\n",
      "Train Epoch: 15 [25600/60000] Loss: 0.096642 Acc: 0.9688\n",
      "Train Epoch: 15 [28800/60000] Loss: 0.205547 Acc: 0.9375\n",
      "Train Epoch: 15 [32000/60000] Loss: 0.150905 Acc: 0.9375\n",
      "Train Epoch: 15 [35200/60000] Loss: 0.097550 Acc: 0.9688\n",
      "Train Epoch: 15 [38400/60000] Loss: 0.172413 Acc: 0.8750\n",
      "Train Epoch: 15 [41600/60000] Loss: 0.100673 Acc: 0.9688\n",
      "Train Epoch: 15 [44800/60000] Loss: 0.136053 Acc: 0.9375\n",
      "Train Epoch: 15 [48000/60000] Loss: 0.245436 Acc: 0.8438\n",
      "Train Epoch: 15 [51200/60000] Loss: 0.381462 Acc: 0.8750\n",
      "Train Epoch: 15 [54400/60000] Loss: 0.218781 Acc: 0.9062\n",
      "Train Epoch: 15 [57600/60000] Loss: 0.353674 Acc: 0.9688\n",
      "Elapsed 224.67s, 14.04 s/epoch, 0.01 s/batch, ets 56.17s\n",
      "\n",
      "Test set: Average loss: 0.2791, Accuracy: 8988/10000 (90%)\n",
      "\n",
      "Train Epoch: 16 [3200/60000] Loss: 0.042453 Acc: 1.0000\n",
      "Train Epoch: 16 [6400/60000] Loss: 0.096557 Acc: 0.9688\n",
      "Train Epoch: 16 [9600/60000] Loss: 0.165121 Acc: 0.9375\n",
      "Train Epoch: 16 [12800/60000] Loss: 0.056455 Acc: 0.9688\n",
      "Train Epoch: 16 [16000/60000] Loss: 0.191187 Acc: 0.9375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 16 [19200/60000] Loss: 0.118893 Acc: 0.9688\n",
      "Train Epoch: 16 [22400/60000] Loss: 0.169708 Acc: 0.9375\n",
      "Train Epoch: 16 [25600/60000] Loss: 0.196629 Acc: 0.9688\n",
      "Train Epoch: 16 [28800/60000] Loss: 0.170096 Acc: 0.9375\n",
      "Train Epoch: 16 [32000/60000] Loss: 0.088359 Acc: 0.9688\n",
      "Train Epoch: 16 [35200/60000] Loss: 0.148288 Acc: 0.9062\n",
      "Train Epoch: 16 [38400/60000] Loss: 0.097679 Acc: 0.9688\n",
      "Train Epoch: 16 [41600/60000] Loss: 0.135702 Acc: 0.9688\n",
      "Train Epoch: 16 [44800/60000] Loss: 0.168817 Acc: 0.9688\n",
      "Train Epoch: 16 [48000/60000] Loss: 0.184305 Acc: 0.9375\n",
      "Train Epoch: 16 [51200/60000] Loss: 0.103263 Acc: 0.9688\n",
      "Train Epoch: 16 [54400/60000] Loss: 0.112196 Acc: 0.9688\n",
      "Train Epoch: 16 [57600/60000] Loss: 0.132767 Acc: 0.9375\n",
      "Elapsed 239.22s, 14.07 s/epoch, 0.01 s/batch, ets 42.22s\n",
      "\n",
      "Test set: Average loss: 0.2964, Accuracy: 8931/10000 (89%)\n",
      "\n",
      "Train Epoch: 17 [3200/60000] Loss: 0.357274 Acc: 0.8750\n",
      "Train Epoch: 17 [6400/60000] Loss: 0.094166 Acc: 0.9375\n",
      "Train Epoch: 17 [9600/60000] Loss: 0.370018 Acc: 0.9062\n",
      "Train Epoch: 17 [12800/60000] Loss: 0.274746 Acc: 0.8438\n",
      "Train Epoch: 17 [16000/60000] Loss: 0.179400 Acc: 0.9375\n",
      "Train Epoch: 17 [19200/60000] Loss: 0.234666 Acc: 0.9062\n",
      "Train Epoch: 17 [22400/60000] Loss: 0.304864 Acc: 0.9062\n",
      "Train Epoch: 17 [25600/60000] Loss: 0.057719 Acc: 0.9688\n",
      "Train Epoch: 17 [28800/60000] Loss: 0.312161 Acc: 0.8438\n",
      "Train Epoch: 17 [32000/60000] Loss: 0.066542 Acc: 1.0000\n",
      "Train Epoch: 17 [35200/60000] Loss: 0.343047 Acc: 0.8438\n",
      "Train Epoch: 17 [38400/60000] Loss: 0.106734 Acc: 0.9688\n",
      "Train Epoch: 17 [41600/60000] Loss: 0.141441 Acc: 0.9688\n",
      "Train Epoch: 17 [44800/60000] Loss: 0.078125 Acc: 0.9688\n",
      "Train Epoch: 17 [48000/60000] Loss: 0.171364 Acc: 0.9375\n",
      "Train Epoch: 17 [51200/60000] Loss: 0.125809 Acc: 0.9688\n",
      "Train Epoch: 17 [54400/60000] Loss: 0.241392 Acc: 0.9062\n",
      "Train Epoch: 17 [57600/60000] Loss: 0.076829 Acc: 0.9688\n",
      "Elapsed 253.54s, 14.09 s/epoch, 0.01 s/batch, ets 28.17s\n",
      "\n",
      "Test set: Average loss: 0.2806, Accuracy: 9007/10000 (90%)\n",
      "\n",
      "Train Epoch: 18 [3200/60000] Loss: 0.044944 Acc: 1.0000\n",
      "Train Epoch: 18 [6400/60000] Loss: 0.236076 Acc: 0.9375\n",
      "Train Epoch: 18 [9600/60000] Loss: 0.076975 Acc: 0.9688\n",
      "Train Epoch: 18 [12800/60000] Loss: 0.201862 Acc: 0.9375\n",
      "Train Epoch: 18 [16000/60000] Loss: 0.152489 Acc: 0.9688\n",
      "Train Epoch: 18 [19200/60000] Loss: 0.184865 Acc: 0.9062\n",
      "Train Epoch: 18 [22400/60000] Loss: 0.147139 Acc: 0.9375\n",
      "Train Epoch: 18 [25600/60000] Loss: 0.259612 Acc: 0.8750\n",
      "Train Epoch: 18 [28800/60000] Loss: 0.376528 Acc: 0.8438\n",
      "Train Epoch: 18 [32000/60000] Loss: 0.065225 Acc: 1.0000\n",
      "Train Epoch: 18 [35200/60000] Loss: 0.224152 Acc: 0.9688\n",
      "Train Epoch: 18 [38400/60000] Loss: 0.251127 Acc: 0.9062\n",
      "Train Epoch: 18 [41600/60000] Loss: 0.158132 Acc: 0.8750\n",
      "Train Epoch: 18 [44800/60000] Loss: 0.082555 Acc: 0.9688\n",
      "Train Epoch: 18 [48000/60000] Loss: 0.295478 Acc: 0.9375\n",
      "Train Epoch: 18 [51200/60000] Loss: 0.166866 Acc: 0.9688\n",
      "Train Epoch: 18 [54400/60000] Loss: 0.071013 Acc: 1.0000\n",
      "Train Epoch: 18 [57600/60000] Loss: 0.161950 Acc: 0.9375\n",
      "Elapsed 267.85s, 14.10 s/epoch, 0.01 s/batch, ets 14.10s\n",
      "\n",
      "Test set: Average loss: 0.2736, Accuracy: 9028/10000 (90%)\n",
      "\n",
      "Train Epoch: 19 [3200/60000] Loss: 0.254402 Acc: 0.9062\n",
      "Train Epoch: 19 [6400/60000] Loss: 0.325240 Acc: 0.8750\n",
      "Train Epoch: 19 [9600/60000] Loss: 0.153207 Acc: 0.9688\n",
      "Train Epoch: 19 [12800/60000] Loss: 0.284934 Acc: 0.9062\n",
      "Train Epoch: 19 [16000/60000] Loss: 0.186738 Acc: 0.9062\n",
      "Train Epoch: 19 [19200/60000] Loss: 0.145419 Acc: 0.9688\n",
      "Train Epoch: 19 [22400/60000] Loss: 0.173815 Acc: 0.9062\n",
      "Train Epoch: 19 [25600/60000] Loss: 0.234767 Acc: 0.8750\n",
      "Train Epoch: 19 [28800/60000] Loss: 0.211100 Acc: 0.9062\n",
      "Train Epoch: 19 [32000/60000] Loss: 0.256639 Acc: 0.8750\n",
      "Train Epoch: 19 [35200/60000] Loss: 0.291410 Acc: 0.8438\n",
      "Train Epoch: 19 [38400/60000] Loss: 0.086591 Acc: 0.9688\n",
      "Train Epoch: 19 [41600/60000] Loss: 0.256481 Acc: 0.8750\n",
      "Train Epoch: 19 [44800/60000] Loss: 0.158507 Acc: 0.9375\n",
      "Train Epoch: 19 [48000/60000] Loss: 0.221825 Acc: 0.9062\n",
      "Train Epoch: 19 [51200/60000] Loss: 0.234578 Acc: 0.9062\n",
      "Train Epoch: 19 [54400/60000] Loss: 0.173424 Acc: 0.9375\n",
      "Train Epoch: 19 [57600/60000] Loss: 0.184749 Acc: 0.9062\n",
      "Elapsed 282.11s, 14.11 s/epoch, 0.01 s/batch, ets 0.00s\n",
      "\n",
      "Test set: Average loss: 0.2769, Accuracy: 9050/10000 (90%)\n",
      "\n",
      "Total time: 285.74, Best Loss: 0.274\n"
     ]
    }
   ],
   "source": [
    "model = LeNet()\n",
    "modelBN = LeNetBN() \n",
    "\n",
    "model, epoch_train_loss, epoch_train_acc, epoch_test_loss, epoch_test_acc = main(model)\n",
    "\n",
    "modelBN, epoch_train_loss_bn, epoch_train_acc_bn, epoch_test_loss_bn, epoch_test_acc_bn = main(modelBN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font style=\"color:blue\">10.Plot Loss</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot loss\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 6)\n",
    "x = range(len(epoch_train_loss))\n",
    "\n",
    "\n",
    "plt.figure\n",
    "plt.plot(x, epoch_train_loss, 'r', label=\"train loss\")\n",
    "plt.plot(x, epoch_test_loss, 'b', label=\"validation loss\")\n",
    "\n",
    "plt.plot(x, epoch_train_loss_bn, 'r--', label=\"train loss with BN\")\n",
    "plt.plot(x, epoch_test_loss_bn, 'b--',label=\"validation loss with BN\")\n",
    "\n",
    "plt.xlabel('epoch no.')\n",
    "plt.ylabel('loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from the above curves that when we use Batch Normalization, the training converges much faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font style=\"color:blue\">Miscellaneous: Calculate Mean and Standard Deviation of Fashion MNIST </font>\n",
    "\n",
    "Ideally, we should not use the same mean and standard deviation for Fashion MNIST and MNIST, even though most people don't care much as it does not have a profound effect on the results.\n",
    "\n",
    "But, we will find those numbers for Fashion MNIST and use it instead of the mean and standard deviation of MNIST. \n",
    "\n",
    "We need to simply find the mean and standard deviation of the whole dataset. So, we load the dataset and find it using the mean and std functions as given below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2860)\n",
      "tensor(0.3530)\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "train_transform = transforms.Compose([transforms.ToTensor()])\n",
    "train_set = torchvision.datasets.FashionMNIST(root=\"./data\", train=True, download=True, transform=train_transform)\n",
    "\n",
    "print(train_set.data.float().mean()/255)\n",
    "print(train_set.data.float().std()/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
