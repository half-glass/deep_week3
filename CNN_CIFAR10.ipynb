{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN_CIFAR10.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/half-glass/deep_week3/blob/master/CNN_CIFAR10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ksh-kBK6bhVm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "606bbd3d-9c85-4aa7-c53e-aca270ee409f"
      },
      "source": [
        "required_training = True\r\n",
        "%matplotlib inline\r\n",
        "import matplotlib.pyplot as plt  # one of the best graphics library for python\r\n",
        "import os\r\n",
        "import time\r\n",
        "\r\n",
        "from typing import Iterable\r\n",
        "from dataclasses import dataclass\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.optim as optim\r\n",
        "import torch.nn.functional as F\r\n",
        "\r\n",
        "from torchvision import datasets, transforms\r\n",
        "import torchvision\r\n",
        "\r\n",
        "def get_mean_std_train_data(data_root):\r\n",
        "    \r\n",
        "    train_transform = transforms.Compose([transforms.ToTensor()])\r\n",
        "    train_set = datasets.CIFAR10(root=data_root, train=True, download=False, transform=train_transform)\r\n",
        "    \r\n",
        "    # return mean (numpy.ndarray) and std (numpy.ndarray)\r\n",
        "    mean = np.array([0.5, 0.5, 0.5])\r\n",
        "    std = np.array([0.5, 0.5, 0.5])\r\n",
        "    \r\n",
        "    ###\r\n",
        "    d = torch.tensor(train_set.data, dtype=float)\r\n",
        "    #print(d.shape)\r\n",
        "    #print(d.dtype)\r\n",
        "    mean = torch.mean(d / 255., [0, 1, 2])\r\n",
        "    std = torch.std(d / 255., [0, 1, 2])\r\n",
        "    #mean = np.array([0., 0., 0.])\r\n",
        "    #std = np.array([1., 1., 1.])\r\n",
        "    #mean = np.array([128., 128., 128.])\r\n",
        "    #std = np.array([25., 25., 25.])\r\n",
        "    ###\r\n",
        "    \r\n",
        "    return mean, std\r\n",
        "\r\n",
        "def get_data(batch_size, data_root, num_workers=1):\r\n",
        "    \r\n",
        "    \r\n",
        "    try:\r\n",
        "        mean, std = get_mean_std_train_data(data_root)\r\n",
        "        assert len(mean) == len(std) == 3\r\n",
        "    except:\r\n",
        "        mean = np.array([0.5, 0.5, 0.5])\r\n",
        "        std = np.array([0.5, 0.5, 0.5])\r\n",
        "        \r\n",
        "    \r\n",
        "    train_test_transforms = transforms.Compose([                     \r\n",
        "        # this re-scale image tensor values between 0-1. image_tensor /= 255\r\n",
        "        transforms.ToTensor(),\r\n",
        "        # subtract mean and divide by variance.\r\n",
        "        transforms.Normalize(mean, std),\r\n",
        "        transforms.RandomChoice([transforms.RandomHorizontalFlip(),\r\n",
        "                                 transforms.RandomRotation(10),\r\n",
        "                                 ])\r\n",
        "    ])\r\n",
        "    \r\n",
        "    # train dataloader\r\n",
        "    train_loader = torch.utils.data.DataLoader(\r\n",
        "        datasets.CIFAR10(root=data_root, train=True, download=True, transform=train_test_transforms),\r\n",
        "        batch_size=batch_size,\r\n",
        "        shuffle=True,\r\n",
        "        num_workers=num_workers\r\n",
        "    )\r\n",
        "    \r\n",
        "    # test dataloader\r\n",
        "    test_loader = torch.utils.data.DataLoader(\r\n",
        "        datasets.CIFAR10(root=data_root, train=False, download=False, transform=train_test_transforms),\r\n",
        "        batch_size=batch_size,\r\n",
        "        shuffle=False,\r\n",
        "        num_workers=num_workers\r\n",
        "    )\r\n",
        "    return train_loader, test_loader\r\n",
        "\r\n",
        "@dataclass\r\n",
        "class SystemConfiguration:\r\n",
        "    '''\r\n",
        "    Describes the common system setting needed for reproducible training\r\n",
        "    '''\r\n",
        "    seed: int = 42  # seed number to set the state of all random number generators\r\n",
        "    cudnn_benchmark_enabled: bool = True  # enable CuDNN benchmark for the sake of performance\r\n",
        "    cudnn_deterministic: bool = True  # make cudnn deterministic (reproducible training)\r\n",
        "\r\n",
        "def setup_system(system_config: SystemConfiguration) -> None:\r\n",
        "    torch.manual_seed(system_config.seed)\r\n",
        "    if torch.cuda.is_available():\r\n",
        "        torch.backends.cudnn_benchmark_enabled = system_config.cudnn_benchmark_enabled\r\n",
        "        torch.backends.cudnn.deterministic = system_config.cudnn_deterministic\r\n",
        "\r\n",
        "@dataclass\r\n",
        "class TrainingConfiguration:\r\n",
        "    '''\r\n",
        "    Describes configuration of the training process\r\n",
        "    '''\r\n",
        "    batch_size: int = 16  # amount of data to pass through the network at each forward-backward iteration\r\n",
        "    epochs_count: int = 2  # number of times the whole dataset will be passed through the network\r\n",
        "    learning_rate: float = 0.1  # determines the speed of network's weights update\r\n",
        "        \r\n",
        "    log_interval: int = 100  # how many batches to wait between logging training status\r\n",
        "    test_interval: int = 1  # how many epochs to wait before another test. Set to 1 to get val loss at each epoch\r\n",
        "    data_root: str = \"../resource/lib/publicdata/images\"  # folder to save data\r\n",
        "    num_workers: int = 2  # number of concurrent processes using to prepare data\r\n",
        "    device: str = 'cuda'  # device to use for training.\r\n",
        "    # update changed parameters in blow coding block.\r\n",
        "    # Please do not change \"data_root\" \r\n",
        "    \r\n",
        "    ###\r\n",
        "    batch_size: int = 64\r\n",
        "    epochs_count: int = 30\r\n",
        "    learning_rate: float = 0.0006\r\n",
        "    ###\r\n",
        "\r\n",
        "def train(\r\n",
        "    train_config: TrainingConfiguration, model: nn.Module, optimizer: torch.optim.Optimizer,\r\n",
        "    train_loader: torch.utils.data.DataLoader, epoch_idx: int\r\n",
        ") -> None:\r\n",
        "    \r\n",
        "    # change model in training mood\r\n",
        "    model.train()\r\n",
        "    \r\n",
        "    # to get batch loss\r\n",
        "    batch_loss = np.array([])\r\n",
        "    \r\n",
        "    # to get batch accuracy\r\n",
        "    batch_acc = np.array([])\r\n",
        "    \r\n",
        "    # total correct\r\n",
        "    correctSum = 0\r\n",
        "\r\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\r\n",
        "        \r\n",
        "        # clone target\r\n",
        "        indx_target = target.clone()\r\n",
        "        # send data to device (its is medatory if GPU has to be used)\r\n",
        "        data = data.to(train_config.device)\r\n",
        "        # send target to device\r\n",
        "        target = target.to(train_config.device)\r\n",
        "\r\n",
        "        # reset parameters gradient to zero\r\n",
        "        optimizer.zero_grad()\r\n",
        "        \r\n",
        "        # forward pass to the model\r\n",
        "        output = model(data)\r\n",
        "        \r\n",
        "        # cross entropy loss\r\n",
        "        loss = F.cross_entropy(output, target)\r\n",
        "        \r\n",
        "        # find gradients w.r.t training parameters\r\n",
        "        loss.backward()\r\n",
        "        # Update parameters using gardients\r\n",
        "        optimizer.step()\r\n",
        "        \r\n",
        "        batch_loss = np.append(batch_loss, [loss.item()])\r\n",
        "        \r\n",
        "        # Score to probability using softmax\r\n",
        "        prob = F.softmax(output, dim=1)\r\n",
        "            \r\n",
        "        # get the index of the max probability\r\n",
        "        pred = prob.data.max(dim=1)[1]  \r\n",
        "                        \r\n",
        "        # correct prediction\r\n",
        "        correct = pred.cpu().eq(indx_target).sum()\r\n",
        "        correctSum += correct\r\n",
        "            \r\n",
        "        # accuracy\r\n",
        "        acc = float(correct) / float(len(data))\r\n",
        "        \r\n",
        "        batch_acc = np.append(batch_acc, [acc])\r\n",
        "\r\n",
        "        #if batch_idx % train_config.log_interval == 0 and batch_idx > 0:              \r\n",
        "        #    print(\r\n",
        "        #        'Train Epoch: {} [{}/{}] Loss: {:.6f} Acc: {:.4f}'.format(\r\n",
        "        #            epoch_idx, batch_idx * len(data), len(train_loader.dataset), loss.item(), acc\r\n",
        "        #        )\r\n",
        "        #    )\r\n",
        "    \r\n",
        "    epoch_loss = batch_loss.mean()\r\n",
        "    epoch_acc = batch_acc.mean()\r\n",
        "\r\n",
        "    print(\r\n",
        "        'Trainset: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\r\n",
        "            epoch_loss, correctSum, len(train_loader.dataset), epoch_acc*100\r\n",
        "        )\r\n",
        "    )\r\n",
        "\r\n",
        "    return epoch_loss, epoch_acc\r\n",
        "\r\n",
        "def validate(\r\n",
        "    train_config: TrainingConfiguration,\r\n",
        "    model: nn.Module,\r\n",
        "    test_loader: torch.utils.data.DataLoader,\r\n",
        ") -> float:\r\n",
        "    model.eval()\r\n",
        "    test_loss = 0\r\n",
        "    count_corect_predictions = 0\r\n",
        "    for data, target in test_loader:\r\n",
        "        indx_target = target.clone()\r\n",
        "        data = data.to(train_config.device)\r\n",
        "        \r\n",
        "        target = target.to(train_config.device)\r\n",
        "        \r\n",
        "        output = model(data)\r\n",
        "        # add loss for each mini batch\r\n",
        "        test_loss += F.cross_entropy(output, target).item()\r\n",
        "        \r\n",
        "        # Score to probability using softmax\r\n",
        "        prob = F.softmax(output, dim=1)\r\n",
        "        \r\n",
        "        # get the index of the max probability\r\n",
        "        pred = prob.data.max(dim=1)[1] \r\n",
        "        \r\n",
        "        # add correct prediction count\r\n",
        "        count_corect_predictions += pred.cpu().eq(indx_target).sum()\r\n",
        "\r\n",
        "    # average over number of mini-batches\r\n",
        "    test_loss = test_loss / len(test_loader)  \r\n",
        "    \r\n",
        "    # average over number of dataset\r\n",
        "    accuracy = 100. * count_corect_predictions / len(test_loader.dataset)\r\n",
        "    \r\n",
        "    print(\r\n",
        "        'Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(\r\n",
        "            test_loss, count_corect_predictions, len(test_loader.dataset), accuracy\r\n",
        "        )\r\n",
        "    )\r\n",
        "    return test_loss, accuracy/100.0\r\n",
        "\r\n",
        "def save_model(model, device, model_dir='models', model_file_name='cifar10_cnn_model.pt'):\r\n",
        "    \r\n",
        "\r\n",
        "    if not os.path.exists(model_dir):\r\n",
        "        os.makedirs(model_dir)\r\n",
        "\r\n",
        "    model_path = os.path.join(model_dir, model_file_name)\r\n",
        "\r\n",
        "    # make sure you transfer the model to cpu.\r\n",
        "    if device == 'cuda':\r\n",
        "        model.to('cpu')\r\n",
        "\r\n",
        "    # save the state_dict\r\n",
        "    torch.save(model.state_dict(), model_path)\r\n",
        "    \r\n",
        "    if device == 'cuda':\r\n",
        "        model.to('cuda')\r\n",
        "    \r\n",
        "    return\r\n",
        "\r\n",
        "def main(system_configuration=SystemConfiguration(), training_configuration=TrainingConfiguration()):\r\n",
        "    \r\n",
        "    # system configuration\r\n",
        "    setup_system(system_configuration)\r\n",
        "\r\n",
        "    # batch size\r\n",
        "    batch_size_to_set = training_configuration.batch_size\r\n",
        "    # num_workers\r\n",
        "    num_workers_to_set = training_configuration.num_workers\r\n",
        "    # epochs\r\n",
        "    epoch_num_to_set = training_configuration.epochs_count\r\n",
        "\r\n",
        "    # if GPU is available use training config, \r\n",
        "    # else lowers batch_size, num_workers and epochs count\r\n",
        "    if torch.cuda.is_available():\r\n",
        "        device = \"cuda\"\r\n",
        "    else:\r\n",
        "        device = \"cpu\"\r\n",
        "        num_workers_to_set = 2\r\n",
        "\r\n",
        "    # data loader\r\n",
        "    train_loader, test_loader = get_data(\r\n",
        "        batch_size=training_configuration.batch_size,\r\n",
        "        data_root=training_configuration.data_root,\r\n",
        "        num_workers=num_workers_to_set\r\n",
        "    )\r\n",
        "    \r\n",
        "    # Update training configuration\r\n",
        "    training_configuration = TrainingConfiguration(\r\n",
        "        device=device,\r\n",
        "        num_workers=num_workers_to_set\r\n",
        "    )\r\n",
        "\r\n",
        "    # initiate model\r\n",
        "    model = MyModel()\r\n",
        "        \r\n",
        "    # send model to device (GPU/CPU)\r\n",
        "    model.to(training_configuration.device)\r\n",
        "\r\n",
        "    # optimizer\r\n",
        "    #optimizer = optim.SGD(\r\n",
        "    #    model.parameters(),\r\n",
        "    #    lr=training_configuration.learning_rate,\r\n",
        "    #    momentum = 0.9,\r\n",
        "    #)\r\n",
        "    optimizer = optim.Adam(\r\n",
        "        model.parameters(),\r\n",
        "        lr=training_configuration.learning_rate,\r\n",
        "    )\r\n",
        "\r\n",
        "    best_loss = torch.tensor(np.inf)\r\n",
        "    best_accuracy = torch.tensor(0)\r\n",
        "    \r\n",
        "    # epoch train/test loss\r\n",
        "    epoch_train_loss = np.array([])\r\n",
        "    epoch_test_loss = np.array([])\r\n",
        "    \r\n",
        "    # epch train/test accuracy\r\n",
        "    epoch_train_acc = np.array([])\r\n",
        "    epoch_test_acc = np.array([])\r\n",
        "    \r\n",
        "    # trainig time measurement\r\n",
        "    t_begin = time.time()\r\n",
        "    for epoch in range(training_configuration.epochs_count):\r\n",
        "        \r\n",
        "        train_loss, train_acc = train(training_configuration, model, optimizer, train_loader, epoch)\r\n",
        "        \r\n",
        "        epoch_train_loss = np.append(epoch_train_loss, [train_loss])\r\n",
        "        \r\n",
        "        epoch_train_acc = np.append(epoch_train_acc, [train_acc])\r\n",
        "\r\n",
        "        elapsed_time = time.time() - t_begin\r\n",
        "        speed_epoch = elapsed_time / (epoch + 1)\r\n",
        "        speed_batch = speed_epoch / len(train_loader)\r\n",
        "        eta = speed_epoch * training_configuration.epochs_count - elapsed_time\r\n",
        "        \r\n",
        "        #print(\r\n",
        "        #    \"Elapsed {:.2f}s, {:.2f} s/epoch, {:.2f} s/batch, ets {:.2f}s\".format(\r\n",
        "        #        elapsed_time, speed_epoch, speed_batch, eta\r\n",
        "        #    )\r\n",
        "        #)\r\n",
        "\r\n",
        "        if epoch % training_configuration.test_interval == 0:\r\n",
        "            current_loss, current_accuracy = validate(training_configuration, model, test_loader)\r\n",
        "            \r\n",
        "            epoch_test_loss = np.append(epoch_test_loss, [current_loss])\r\n",
        "        \r\n",
        "            epoch_test_acc = np.append(epoch_test_acc, [current_accuracy])\r\n",
        "            \r\n",
        "            if current_loss < best_loss:\r\n",
        "                best_loss = current_loss\r\n",
        "            \r\n",
        "            if current_accuracy > best_accuracy:\r\n",
        "                best_accuracy = current_accuracy\r\n",
        "                print('Accuracy improved, saving the model.\\n')\r\n",
        "                save_model(model, device)\r\n",
        "            \r\n",
        "                \r\n",
        "    print(\"Total time: {:.2f}, Best Loss: {:.3f}, Best Accuracy: {:.3f}\".format(time.time() - t_begin, best_loss, \r\n",
        "                                                                                best_accuracy))\r\n",
        "    \r\n",
        "    return model, epoch_train_loss, epoch_train_acc, epoch_test_loss, epoch_test_acc\r\n",
        "\r\n",
        "class MyModel(nn.Module):\r\n",
        "    def __init__(self):\r\n",
        "        super().__init__()\r\n",
        "\r\n",
        "        ###\r\n",
        "        k = 3\r\n",
        "        p = 1\r\n",
        "        z1 = 256 #32 #64\r\n",
        "        z2 = 512 #32 #128\r\n",
        "        self._body = nn.Sequential(\r\n",
        "            # input size = (32, 32, 3)\r\n",
        "            \r\n",
        "            # 32 --> imSize /= 2 --> 16\r\n",
        "            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=k, padding=p, padding_mode='replicate'),\r\n",
        "            nn.ReLU(inplace=True),\r\n",
        "            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=k, padding=p, padding_mode='replicate'),\r\n",
        "            nn.BatchNorm2d(32),\r\n",
        "            nn.ReLU(inplace=True),\r\n",
        "            nn.MaxPool2d(kernel_size=2),\r\n",
        "\r\n",
        "            # 16 --> imSize /= 2 --> 8\r\n",
        "            nn.Conv2d(in_channels=32, out_channels=z1, kernel_size=k, padding=p, padding_mode='replicate'),\r\n",
        "            nn.ReLU(inplace=True),\r\n",
        "            nn.Conv2d(in_channels=z1, out_channels=z1, kernel_size=k, padding=p, padding_mode='replicate'),\r\n",
        "            nn.BatchNorm2d(z1),\r\n",
        "            nn.ReLU(inplace=True),\r\n",
        "            nn.MaxPool2d(kernel_size=2),\r\n",
        "            nn.Dropout(0.5),\r\n",
        "\r\n",
        "            # 8 --> imSize /= 2 --> 4\r\n",
        "            nn.Conv2d(in_channels=z1, out_channels=z1, kernel_size=1),\r\n",
        "            nn.ReLU(inplace=True),\r\n",
        "            nn.Conv2d(in_channels=z1, out_channels=z2, kernel_size=k, padding=p, padding_mode='replicate'),\r\n",
        "            nn.ReLU(inplace=True),\r\n",
        "            nn.Conv2d(in_channels=z2, out_channels=z2, kernel_size=1),\r\n",
        "            nn.ReLU(inplace=True),\r\n",
        "            nn.Conv2d(in_channels=z2, out_channels=z2, kernel_size=k, padding=p, padding_mode='replicate'),\r\n",
        "            nn.BatchNorm2d(z2),\r\n",
        "            nn.ReLU(inplace=True),\r\n",
        "            nn.MaxPool2d(kernel_size=2),\r\n",
        "            nn.Dropout(0.5),\r\n",
        "\r\n",
        "            #nn.BatchNorm2d(32),\r\n",
        "        )\r\n",
        "        \r\n",
        "        self._head = nn.Sequential(\r\n",
        "            nn.Linear(in_features=z2*4*4, out_features=120),\r\n",
        "            nn.ReLU(inplace=True),\r\n",
        "            \r\n",
        "            #nn.Linear(in_features=120, out_features=84),\r\n",
        "            #nn.ReLU(inplace=True),\r\n",
        "            \r\n",
        "            nn.Linear(in_features=120, out_features=10)\r\n",
        "        )\r\n",
        "        ###\r\n",
        "        \r\n",
        "    def forward(self, x):\r\n",
        "        ###\r\n",
        "        x = self._body(x)\r\n",
        "        x = x.view(x.size()[0], -1)\r\n",
        "        x = self._head(x)\r\n",
        "        ###\r\n",
        "        \r\n",
        "        return x\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "my_model = MyModel()\r\n",
        "#print(my_model)\r\n",
        "\r\n",
        "if required_training:\r\n",
        "    model, epoch_train_loss, epoch_train_acc, epoch_test_loss, epoch_test_acc = main()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Trainset: Average loss: 1.4077, Accuracy: 24077/50000 (48%)\n",
            "Test set: Average loss: 1.1357, Accuracy: 5857/10000 (59%)\n",
            "Accuracy improved, saving the model.\n",
            "\n",
            "Trainset: Average loss: 1.0117, Accuracy: 31882/50000 (64%)\n",
            "Test set: Average loss: 1.2430, Accuracy: 5861/10000 (59%)\n",
            "Accuracy improved, saving the model.\n",
            "\n",
            "Trainset: Average loss: 0.8546, Accuracy: 34782/50000 (70%)\n",
            "Test set: Average loss: 0.8223, Accuracy: 7148/10000 (71%)\n",
            "Accuracy improved, saving the model.\n",
            "\n",
            "Trainset: Average loss: 0.7648, Accuracy: 36457/50000 (73%)\n",
            "Test set: Average loss: 0.8416, Accuracy: 7142/10000 (71%)\n",
            "Trainset: Average loss: 0.6933, Accuracy: 37759/50000 (76%)\n",
            "Test set: Average loss: 0.6833, Accuracy: 7646/10000 (76%)\n",
            "Accuracy improved, saving the model.\n",
            "\n",
            "Trainset: Average loss: 0.6476, Accuracy: 38722/50000 (77%)\n",
            "Test set: Average loss: 0.6193, Accuracy: 7844/10000 (78%)\n",
            "Accuracy improved, saving the model.\n",
            "\n",
            "Trainset: Average loss: 0.6044, Accuracy: 39510/50000 (79%)\n",
            "Test set: Average loss: 0.6967, Accuracy: 7586/10000 (76%)\n",
            "Trainset: Average loss: 0.5672, Accuracy: 40071/50000 (80%)\n",
            "Test set: Average loss: 0.6319, Accuracy: 7918/10000 (79%)\n",
            "Accuracy improved, saving the model.\n",
            "\n",
            "Trainset: Average loss: 0.5295, Accuracy: 40679/50000 (81%)\n",
            "Test set: Average loss: 0.5658, Accuracy: 8109/10000 (81%)\n",
            "Accuracy improved, saving the model.\n",
            "\n",
            "Trainset: Average loss: 0.5049, Accuracy: 41197/50000 (82%)\n",
            "Test set: Average loss: 0.5677, Accuracy: 8112/10000 (81%)\n",
            "Accuracy improved, saving the model.\n",
            "\n",
            "Trainset: Average loss: 0.4830, Accuracy: 41614/50000 (83%)\n",
            "Test set: Average loss: 0.5328, Accuracy: 8147/10000 (81%)\n",
            "Accuracy improved, saving the model.\n",
            "\n",
            "Trainset: Average loss: 0.4572, Accuracy: 42064/50000 (84%)\n",
            "Test set: Average loss: 0.5057, Accuracy: 8325/10000 (83%)\n",
            "Accuracy improved, saving the model.\n",
            "\n",
            "Trainset: Average loss: 0.4316, Accuracy: 42490/50000 (85%)\n",
            "Test set: Average loss: 0.5784, Accuracy: 8055/10000 (81%)\n",
            "Trainset: Average loss: 0.4149, Accuracy: 42784/50000 (86%)\n",
            "Test set: Average loss: 0.4693, Accuracy: 8441/10000 (84%)\n",
            "Accuracy improved, saving the model.\n",
            "\n",
            "Trainset: Average loss: 0.3938, Accuracy: 43115/50000 (86%)\n",
            "Test set: Average loss: 0.5149, Accuracy: 8293/10000 (83%)\n",
            "Trainset: Average loss: 0.3801, Accuracy: 43492/50000 (87%)\n",
            "Test set: Average loss: 0.4603, Accuracy: 8474/10000 (85%)\n",
            "Accuracy improved, saving the model.\n",
            "\n",
            "Trainset: Average loss: 0.3607, Accuracy: 43775/50000 (88%)\n",
            "Test set: Average loss: 0.4333, Accuracy: 8549/10000 (85%)\n",
            "Accuracy improved, saving the model.\n",
            "\n",
            "Trainset: Average loss: 0.3466, Accuracy: 43983/50000 (88%)\n",
            "Test set: Average loss: 0.4202, Accuracy: 8586/10000 (86%)\n",
            "Accuracy improved, saving the model.\n",
            "\n",
            "Trainset: Average loss: 0.3351, Accuracy: 44181/50000 (88%)\n",
            "Test set: Average loss: 0.4654, Accuracy: 8424/10000 (84%)\n",
            "Trainset: Average loss: 0.3211, Accuracy: 44377/50000 (89%)\n",
            "Test set: Average loss: 0.4181, Accuracy: 8603/10000 (86%)\n",
            "Accuracy improved, saving the model.\n",
            "\n",
            "Trainset: Average loss: 0.3117, Accuracy: 44495/50000 (89%)\n",
            "Test set: Average loss: 0.4316, Accuracy: 8588/10000 (86%)\n",
            "Trainset: Average loss: 0.2996, Accuracy: 44722/50000 (89%)\n",
            "Test set: Average loss: 0.4085, Accuracy: 8660/10000 (87%)\n",
            "Accuracy improved, saving the model.\n",
            "\n",
            "Trainset: Average loss: 0.2793, Accuracy: 45188/50000 (90%)\n",
            "Test set: Average loss: 0.4254, Accuracy: 8631/10000 (86%)\n",
            "Trainset: Average loss: 0.2780, Accuracy: 45228/50000 (90%)\n",
            "Test set: Average loss: 0.4262, Accuracy: 8648/10000 (86%)\n",
            "Trainset: Average loss: 0.2606, Accuracy: 45473/50000 (91%)\n",
            "Test set: Average loss: 0.4202, Accuracy: 8668/10000 (87%)\n",
            "Accuracy improved, saving the model.\n",
            "\n",
            "Trainset: Average loss: 0.2572, Accuracy: 45588/50000 (91%)\n",
            "Test set: Average loss: 0.4224, Accuracy: 8635/10000 (86%)\n",
            "Trainset: Average loss: 0.2474, Accuracy: 45700/50000 (91%)\n",
            "Test set: Average loss: 0.4104, Accuracy: 8716/10000 (87%)\n",
            "Accuracy improved, saving the model.\n",
            "\n",
            "Trainset: Average loss: 0.2399, Accuracy: 45795/50000 (92%)\n",
            "Test set: Average loss: 0.4020, Accuracy: 8739/10000 (87%)\n",
            "Accuracy improved, saving the model.\n",
            "\n",
            "Trainset: Average loss: 0.2297, Accuracy: 45988/50000 (92%)\n",
            "Test set: Average loss: 0.4086, Accuracy: 8741/10000 (87%)\n",
            "Accuracy improved, saving the model.\n",
            "\n",
            "Trainset: Average loss: 0.2270, Accuracy: 46011/50000 (92%)\n",
            "Test set: Average loss: 0.3805, Accuracy: 8774/10000 (88%)\n",
            "Accuracy improved, saving the model.\n",
            "\n",
            "Total time: 821.46, Best Loss: 0.381, Best Accuracy: 0.877\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}